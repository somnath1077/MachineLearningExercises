\section*{Exercise 3.1}

Let $\samplecomp{\epsilon}{\delta}$ be the sample complexity of a PAC-learnable hypothesis class 
$\mathcal{H}$ for a binary classification task. For a fixed $\delta$, let 
$0 < \epsilon_1 \leq \epsilon_2 < 1$ and suppose that 
$\samplecomp{\epsilon_1}{\delta} < \samplecomp{\epsilon_2}{\delta}$. Then when 
running the learning algorithm on $\samplecomp{\epsilon_1}{\delta}$ i.i.d examples, 
we obtain a hypothesis $h$, which with probability at least $1 - \delta$ has 
a true error $\pacerror{h} \leq \epsilon_1 \leq \epsilon_2$. This
implies that for the $(\epsilon_2, \delta)$ combination of parameters, we can bound 
the true error of $h$ by $\epsilon_2$ by using a smaller number of i.i.d examples 
than $\samplecomp{\epsilon_2}{\delta}$. This contradicts
the minimality of the sample complexity function. Hence we must have 
$\samplecomp{\epsilon_1}{\delta} \geq \samplecomp{\epsilon_2}{\delta}$.

Next suppose that $0 < \delta_1 \leq \delta_2 < 1$ and that 
$\samplecomp{\epsilon}{\delta_1} < \samplecomp{\epsilon}{\delta_2}$, where $\epsilon$
is fixed in advance. Then with $\samplecomp{\epsilon}{\delta_1}$ i.i.d examples, the
learner outputs a hypothesis $h$ which with probability at least 
$1 - \delta_1 \geq 1 - \delta_2$ has a true error of at most $\epsilon$. This
implies that for the $(\epsilon, \delta_2)$ combination of parameters, we can bound 
the true error of $h$ by $\epsilon$ by using a smaller number of i.i.d examples 
than $\samplecomp{\epsilon}{\delta_2}$. This again contradicts
the minimality of the sample complexity function. Hence we must have 
$\samplecomp{\epsilon}{\delta_1} \geq \samplecomp{\epsilon}{\delta_2}$.

\section*{Exercise 3.2}

Given a sample $S$, we output a hypothesis $h_S$ with the property that 
$\forall x \in S_x$, 
\[
    h_S(x) = \left \{ \begin{array}{rl} 
                            1, & \text{if $(x, 1) \in S$} \\
                            0, & \text{otherwise}
                      \end{array} \right .
\]
For any sample $S$, this hypothesis has an empirical loss of $0$. Note 
that $h_S$ disagrees with the true labeling function $f$ in at most one point 
$z \in \mathcal{X}$. It's true loss is therefore 
$\Pr_{x \sim \mathcal{D}} \{ f(x) \neq h_S(x)\} = \Pr_{\mathcal{D}} \{z\} := p_z$. 

The true loss of $h_S$ will be $0$ if $(z, 1) \in S$. Therefore the probability 
of getting a ``bad'' sample is $\Pr_{S \sim \mathcal{D}^m}\{(z, 1) \notin S\}$.
Let $z^{*} \in \mathcal{X}$ be a point at which $(1 - p_z)^m$ is maximized. Since 
$(1 - p_{z^{*}})^m \leq e^{- m p_{z^{*}}}$ and since we want the probability of 
picking a bad sample to be at most $\delta$, we want $e^{- m p_{z^{*}}} < \delta$,
which gives us the sample size to be:
\begin{equation}
\label{3.2_samplecomp}
	m  > \frac{\log (1 / \delta)}{p_{z^{*}}}
\end{equation}

Depending on the value of the error bound $\epsilon$, there are two situations to consider. If $\epsilon \geq p_{z^{*}}$, then even a sample of size one will guarantee 
that the true error of $h_s$ is at most $\epsilon$. However if 
$\epsilon < p_{z^{*}}$ then we can then use this in~(\ref{3.2_samplecomp})
to obtain: 
\[
    m > \frac{\log (1 / \delta)}{\epsilon}.
\]
Thus the sample complexity is $\samplecomp{\epsilon}{\delta} = 
\max \left \{1, \frac{\log (1 / \delta)}{\epsilon} \right \}$.

\section*{Exercise 3.3}

Here $\dom = \Rtwo$ and $\range = \{0, 1\}$. The hypothesis class $\hypclass$ is the set of concentric circles in $\Rtwo$ centered at the origin. Assuming realizability, this implies that the true labeling function~$f = h_r$ for some $r \in \Rpos$. Thus $f$ assigns the label $1$ to any point $(x, y) $ that is within a distance of $r$ from the origin and $0$ otherwise. 

Given any sample~$S$, let $q \in \Rpos$ be the minimum real number such that all $(x, y) \in S_x$ with a label of $1$ are included in a circle centered at the origin with radius~$q$. The output of the ERM procedure is $h_q$. The empirical error of $h_q$ is zero, but it's true error is:
\[
	\Prtwo{(x, y) \sim \dist}{(x, y) \in C_r \setminus C_q}
\]
where $C_r$ and $C_q$ are concentric circles centered at the origin with radius~$r$ and~$q$ respectively. Given an $\epsilon > 0$, let $t \in \Rpos$ be such that 
$$\epsilon = \Prtwo{(x, y) \sim \dist}{(x, y) \in C_r \setminus C_t}.$$ 
That is, we choose~$t$ so that the true error matches the probability of picking anything inside the ring described by the circles $C_r$ and $C_t$. Then the probability that we fail to choose any point in this ring in an i.i.d sample of size~$m$ is $(1 - \epsilon)^m \leq e^{- \epsilon m}$. This is the probability that we are handed a ``bad'' sample. Upper bounding this by $\delta$, we obtain that $m > \log(1 / \delta) / \epsilon$.

Now a sample of size at least $\log(1 / \delta) / \epsilon$ has with probability at least $1 - \delta$ a point from $C_r \setminus C_t$, and hence the true error of the resulting ERM hypothesis is at most $\epsilon$. Hence the sample complexity is upper bounded by $\ceiling{\log(1 / \delta) / \epsilon}$.

\section*{Exercise 3.4}

In this example, $\dom = \{0, 1\}^d$, $\range = \{0, 1\}$ and the hypothesis class~$\hypclass$ is the set of all conjunctions over $d$ Boolean variables. Since there are $\sum_{i = 0}^{d} {d \choose i} 2^i = 3^{d}$ Boolean conjunctions over $d$ Boolean variables, the hypothesis class is finite. Hence the sample complexity is 
\begin{align*}
	\samplecomp{\epsilon}{\delta} & = \ceiling{\frac{\log (\hypclass / \delta)}{\epsilon}} \\
	& = \ceiling{\frac{d \cdot \log 3 + \log (1 / \delta)}{\epsilon}}
\end{align*}

To prove that the class $\hypclass$ is PAC-learnable, it suffices to exhibit a polynomial-time algorithm that implements the ERM rule. The algorithm outlined in Figure~\ref{fig:pac_boolean_conjunctions} starts with the formula $x_1 \land \bar{x}_1 \land \cdots \land x_d \land \bar{x}_d$. It runs through the positive examples in the sample $S$ and for each such example, it adjusts the formula so that it satisfies the assignment given in the example. At the end of this procedure, the modified formula satisfies all positive examples of $S$. The time taken is $O(d \cdot |S|)$. 

What may not be immediately apparent is that the formula returned by the algorithm satisfies all negative examples too. This is clear when the sample $S$ has \emph{no} positive examples to begin with as every assignment to $x_1 \land \bar{x}_1 \land \cdots 
            \land x_d \land \bar{x}_d$ results in a $0$. The point is that if there is even \emph{one} positive example, for each $1 \leq i \leq d$, the algorithm eliminates either $x_i$ or $\bar{x}_i$ depending on the assignment. That is, it eliminates half of the literals on seeing that one example and the modified formula $f$ contains the literals of the true labeling function along with possibly others. 
Now the literals of the true labeling function produce a $0$ on all negative examples and so does $f$. Hence the sampling error of the function returned by the algorithm is $0$.   
\begin{figure}
    \begin{algorithmic}[0] % The number tells where the line numbering should start
     \Procedure{PACBoolean}{$S$} 
         \Comment{$S$ is the sample set with elements 
         $\angular{(a_1, \ldots, a_d), b}$, 
         where $(a_1, \ldots, a_d) \in \{0, 1\}^d$ and $b \in \{0, 1\}$}
            \State $f \gets x_1 \land \bar{x}_1 \land \cdots 
            \land x_d \land \bar{x}_d$
            \For{each $\angular{(a_1, \ldots, a_d), b} \in S$ with $b = 1$}
            	\For{$j$ in $[1, \ldots, d]$}
            		\If{$a_j = 1$}
            			\State Delete $\bar{x}_j$ from $f$, if it exists in the formula
            		\Else
            			\State Delete $x_j$ from $f$, if it exists in the formula
            		\EndIf
             	\EndFor
            \EndFor
            \State \textbf{return} $f$
        \EndProcedure
    \end{algorithmic}
\caption{Learning Boolean conjunctions}
\label{fig:pac_boolean_conjunctions}
\end{figure}

