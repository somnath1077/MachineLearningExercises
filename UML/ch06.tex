\chapter{The VC-Dimension}

\section*{Notes on Chapter 6}

We know that finite hypothesis classes are agnostic PAC learnable (and hence
PAC learnable).  What about infinite hypothesis classes? The first example is 
that of an infinite hypothesis class that is PAC learnable. 

\begin{example}[Threshold Functions]
Let $\dom = [0, 1]$ and $\range = \{0, 1\}$. For $r \in [0, 1]$, define 
$h_r \colon \dom \rightarrow \range $ as:
\[
    h_r(x) = \left \{ \begin{array}{ll} 
                        0 & \text{if } x \leq r \\
                        1 & \text{if } x > r
                      \end{array}\right .
\]
Let $\hypclass_{\text{thr}}$ be the set of all threshold functions $h_r$ 
for $r \in [0, 1]$. Since $\hypclass_{\text{thr}}$ is not finite, it is 
not immediately obvious whether it is PAC learnable (in the realizable case). 

Fix $\epsilon, \delta \in (0, 1)$. Let $f = h_s$ be the true labeling function
where $s \in [0, 1]$ and let $\dist$ be the underlying distribution over the
domain $[0, 1]$. Let $s_0 \in [0, s)$ and $s_1 \in [s, 1]$ be numbers such that  
\[
    \dist \left \{ x \in [s_0, s) \right \} = \epsilon =  
    \dist \left \{ x \in [s, s_1] \right \}
\]
If $\dist \left \{ [0, s) \right \} < \epsilon$, then set $s_0 = 0$; similarly, 
if $\dist \left \{ [s, 1] \right \} < \epsilon$, set $s_1 = 1$. Since $\dist$ 
is a distribution, it must place a probability mass of $\epsilon$ either to 
the left or to the right of $s$. 

Given a sample $S$, let $t_0 = \max \{t \colon (t, 0) \in S\}$ and $t_1 = \min
\{t \colon (t, 1) \in S\}$. The ERM algorithm outputs $h_p$, where $p \in (t_0,
t_1)$.  In particular, if the sample presented to the ERM algorithm is such
that $s_0 \leq t_0$ and $t_1 \leq s_1$, then hypothesis $h_p$ returned by the ERM
algorithm will incur a loss of $L_{\dist}(h_p) \leq \epsilon$.

Thus the probability that the hypothesis $\ERM(S)$ output by the ERM algorithm 
has a loss greater than $\epsilon$ on a sample $S$ of size $m$ is:
\begin{align*}
    \Prtwo{S \sim \dist^m}{L_{\dist} (\ERM(S)) > \epsilon} 
     & = \Prtwo{S \sim \dist^m}{S \colon t_0 < s_0 \vee s_1 < t_1} \\
     & \leq  \Prtwo{S \sim \dist^m}{S \colon S|_x \cap [s_0, s) = \emptyset} + 
             \Prtwo{S \sim \dist^m}{S \colon S|_x \cap [s, s_1] = \emptyset} \\      
     & \leq 2 \cdot (1 - \epsilon)^m \\
     & \leq 2 \cdot e^{- \epsilon m}
\end{align*}
Setting the last expression to be at most $\delta$, we obtain that 
$m > \frac{1}{\epsilon} \cdot \log \frac{2}{\delta}$. Hence if we have samples 
of size at least $\frac{1}{\epsilon} \cdot \log \frac{2}{\delta}$, 
\[
    \Prtwo{S \sim \dist^m}{L_{\dist} (\ERM(S)) \leq \epsilon} \geq 1 - \delta, 
\]
which is the condition for PAC learnability.
\end{example}

The second example shows that there are infinite hypothesis classes that are not
PAC learnable at least by using an ERM strategy.

\begin{example}[Identity Function for Finite Sets] 
Let $\dom = \Rone$ and $\range = \{0, 1\}$. Given a set $A \subseteq \dom$,
define $h_A$ as follows:
\[
    h_A = \left \{ \begin{array}{ll} 
                        1 & \text{if } x \in A \\
                        0 & \text{otherwise}
                   \end{array}\right .
\]
Let $\hypclass_{\text{finite}}$ be the set of all such functions $h_A$ for \emph{finite} 
subsets $A$ of $\Rone$ along with the function $h_{1}$ which maps every point in $\Rone$ 
to $1$. We claim that $\hypclass_{\text{finite}}$ is not PAC learnable by an ERM algorithm. 

Consider the case when the true labeling function $f = h_1$, the all-ones
function on $\Rone$ and $\dist$ is the uniform distribution on $[0, 1]$. Since
$f \in \hypclass_{\text{finite}}$, we are assuming that the hypothesis class is
realizable. Fix any sample size $m$. A sample $S$ in this case looks like
$\{(x_1, 1), \ldots, (x_m, 1)\}$ and an obvious ERM strategy is to output $h_A$
for $A = \{x_1, \ldots, x_m\}$. Clearly $L_S (h_A) = 0$ but $L_{\dist} (h_A) =
1$.  
\end{example} 

The previous examples show that the size of the hypothesis class does not characterize
whether it is learnable. This characterization is provided by the so-called VC-dimension. 


