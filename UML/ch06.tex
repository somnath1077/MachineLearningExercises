\chapter{The VC-Dimension}

\section*{Notes on Chapter 6}

We know that finite hypothesis classes are agnostic PAC learnable (and hence
PAC learnable).  What about infinite hypothesis classes? The first example is 
that of an infinite hypothesis class that is PAC learnable. 

\begin{example}[Threshold Functions]
Let $\dom = [0, 1]$ and $\range = \{0, 1\}$. For $r \in [0, 1]$, define 
$h_r \colon \dom \rightarrow \range $ as:
\[
    h_r(x) = \left \{ \begin{array}{ll} 
                        0 & \text{if } x \leq r \\
                        1 & \text{if } x > r
                      \end{array}\right .
\]
Let $\hypclass_{\text{thr}}$ be the set of all threshold functions $h_r$ 
for $r \in [0, 1]$. Since $\hypclass_{\text{thr}}$ is not finite, it is 
not immediately obvious whether it is PAC learnable (in the realizable case). 

Fix $\epsilon, \delta \in (0, 1)$. Let $f = h_s$ be the true labeling function
where $s \in [0, 1]$ and let $\dist$ be the underlying distribution over the
domain $[0, 1]$. Define $q_{\epsilon}$ to the smallest number in $[0, s]$ such
that 
\[
    \dist \left \{ x \in [q_{\epsilon}, s] \right \} = \epsilon.
\]

Given a sample $S$, the ERM algorithm outputs $h_p$, where $p$ is the rightmost
point in the sample with label $0$.  In particular, if the sample presented to
the ERM algorithm contained a point from $[q_{\epsilon}, s]$, and if $p$ were the
rightmost such point, the ERM algorithm will return $h_p$ and incur a loss of:
\[
    L_{\dist}(h_p) = \Prtwo{x \sim \dist}{x \in [p, s]} \leq \epsilon.
\]  

Thus the probability that the hypothesis $\ERM(S)$ output by the ERM algorithm 
has a loss greater than $\epsilon$ on a sample $S$ of size $m$ is:
\begin{align*}
    \Prtwo{S \sim \dist^m}{L_{\dist} (\ERM(S)) > \epsilon} 
     & = \Prtwo{S \sim \dist^m}{S \colon S|_x \cup [q_{\epsilon}, s] = \emptyset} \\     
     & \leq (1 - \epsilon)^m \\
     & \leq e^{- \epsilon m}
\end{align*}
Setting the last expression to be at most $\delta$, we obtain that 
$m > \frac{1}{\epsilon} \cdot \log \frac{1}{\delta}$. Hence if we have samples 
of size at least $\frac{1}{\epsilon} \cdot \log \frac{1}{\delta}$, 
\[
    \Prtwo{S \sim \dist^m}{L_{\dist} (\ERM(S)) \leq \epsilon} \geq 1 - \delta, 
\]
which is the condition for PAC learnability.
\end{example}

The second example shows that there are infinite hypothesis classes that are not
PAC learnable at least by using an ERM strategy.

\begin{example}[Identity Function for Finite Sets] 
Let $\dom = \Rone$ and $\range = \{0, 1\}$. Given a set $A \subseteq \dom$,
define $h_A$ as follows:
\[
    h_A = \left \{ \begin{array}{ll} 
                        1 & \text{if } x \in A \\
                        0 & \text{otherwise}
                   \end{array}\right .
\]
Let $\hypclass_{\text{finite}}$ be the set of all such functions $h_A$ for \emph{finite} 
subsets $A$ of $\Rone$ along with the function $h_{1}$ which maps every point in $\Rone$ 
to $1$. We claim that $\hypclass_{\text{finite}}$ is not PAC learnable by an ERM algorithm. 

Consider the case when the true labeling function $f = h_1$, the all-ones
function on $\Rone$ and $\dist$ is the uniform distribution on $[0, 1]$. Since
$f \in \hypclass_{\text{finite}}$, we are assuming that the hypothesis class is
realizable. Fix any sample size $m$. A sample $S$ in this case looks like
$\{(x_1, 1), \ldots, (x_m, 1)\}$ and an obvious ERM strategy is to output $h_A$
for $A = \{x_1, \ldots, x_m\}$. Clearly $L_S (h_A) = 0$ but $L_{\dist} (h_A) =
1$.  
\end{example} 


