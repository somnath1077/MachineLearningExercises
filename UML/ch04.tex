\chapter{Learning via Uniform Convergence}

\section*{Notes on Chapter 4}

Given any hypothesis class $\hypclass$ and a domain $Z = \dom \times Y$, let
$l$ be a loss function from $\hypclass \times Z \rightarrow \Rpos$. Let
$\dist$ be a distribution over the domain $Z$. The risk of a hypothesis $h \in
\hypclass$ is
\[
    L_{\dist}(h) = \Exptwo{z \sim \dist}{l(h, z)}
\]
The empirical risk of a hypothesis~$h$ w.r.t a sample~$S = \{ z_i \}_{i = 1}^{m}$
is defined as:
\[
    L_{S} (h) = \frac{1}{m} \sum_{i = 1}^m l(h, z_i)
\]

\begin{definition}
A training set $S$ is $\epsilon$-representative w.r.t the domain~$Z$,
the hypothesis class~$\hypclass$, the distribution~$\dist$ and
the loss function~$l$ if for all $h \in \hypclass$,
$|L_S (h) - L_{\dist} (h)| \leq \epsilon$.
\end{definition}
Thus any hypothesis on an $\epsilon$-representative training set has an
in-sample error that is close to their true risk.

If $S$ is $\epsilon$-representative, then the $\ERM_{\hypclass}(S)$ learning
rule is guaranteed to return a good hypothesis. More specifically,
\begin{lemma}
\label{lemma:epsilon_representative}
Fix a hypothesis class $\hypclass$, a domain $Z = \dom \times Y$, a loss
function $l \colon \hypclass \times Z \rightarrow \Rpos$ and a distribution
$\dist$ over the domain $Z$. Let $S$ be an $\epsilon/2$-representative sample.
Then any output $h_S$ of $\ERM_{\hypclass}(S)$ satisfies
\[
    L_{\dist} (h_S) \leq \min_{h' \in \hypclass} L_{\dist}(h') + \epsilon
\]
\end{lemma}
\begin{proof}
The output $h_S$ of $\ERM_{\hypclass}(S)$ is such that
\[
    h_S = \argmin_{h \in \hypclass} L_S(h).
\]
Since $S$ is $\epsilon / 2$ representative,
\begin{align*}
    L_{\dist}(h_S) & \leq L_S (h_S) + \frac{\epsilon}{2} \\
                   & \leq \min_{h \in \hypclass} L_S (h) + \frac{\epsilon}{2} \\
                   & \leq \min_{h \in \hypclass} L_{\dist} (h) + \frac{\epsilon}{2} + \frac{\epsilon}{2} \\
                   & \leq \min_{h \in \hypclass} L_{\dist} (h) + \epsilon.
\end{align*}
In the above derivation, the second inequality follows from the fact that $h_S$ minimizes
the empirical risk among all hypotheses in~$\hypclass$; the third inequality follows
from the fact that $S$ is $\epsilon / 2$-representative and hence
$L_S(h) \leq L_{\dist} (h) + \epsilon / 2$ for all $h \in \hypclass$.
\end{proof}

Therefore in order for the $\ERM$ rule to be an agnostic PAC-learner, all we
need to do is to ensure that with probability of at least $1 - \delta$ over
random choices of the training set, we end up with an
$\epsilon/2$-representative training sample. This requirement is baked into
the definition of \emph{uniform convergence}.

\begin{definition}
A hypothesis class $\hypclass$ is uniformly convergent wrt a domain $Z$
and a loss function $l$, if there exists a function
$\USampleComp \colon (0, 1) \times (0, 1) \rightarrow \Nat$ such that
for all $\epsilon, \delta \in (0, 1)$ and all distributions $\dist$ on $Z$,
if a sample with at least $\usamplecomp{\epsilon}{\delta}$ examples is chosen
i.i.d from $\dist$, then with probability $1 - \delta$, the sample is
$\epsilon$-representative.
\end{definition}

By Lemma~(\ref{lemma:epsilon_representative}), if $\hypclass$ is uniformly
convergent with function $\USampleComp$, then it is agnostically PAC-learnable
with sample complexity $\SampleComp{\epsilon}{\delta} \leq
\usamplecomp{\epsilon / 2}{\delta}$. In this case, the $\ERM$ paradigm is a
successful agnostic PAC-learner for $\hypclass$.
\begin{corollary}
If a class $\hypclass$ is uniformly convergent with sample complexity function
$\USampleComp$ then the class is agnostically PAC-learnable with sample complexity
$\SampleComp{\epsilon}{\delta} \leq \usamplecomp{\epsilon / 2}{\delta}$.
\end{corollary}

The other main result is that all finite hypothesis classes are uniformly convergent
and hence agnostic PAC learnable.
\begin{theorem}
Let  $Z = \dom \times \range$ be a domain,
$\hypclass = \{h \colon \dom \rightarrow \range \}$ be a finite hypothesis class and
let $l \colon \hypclass \times Z \rightarrow [0, 1]$ be a loss function that is
bounded so that $a \leq l(h, z) \leq b$ for all $h \in \hypclass$ and $z \in Z$
for some $a, b \in \Rpos$.
Then $\hypclass$ is uniformly convergent with sample complexity
\[
	\usamplecomp{\epsilon}{\delta} \leq
        \frac{(b - a)^2 \cdot \log (2 |\hypclass| / d)}{2 \epsilon^2}.
\]
Furthermore, $\hypclass$ is agnostically PAC-learnable with sample complexity
\[
	\SampleComp{\epsilon}{\delta} \leq
        \frac{2 \cdot (b - a)^2 \cdot \log (2 |\hypclass| / d)}{\epsilon^2}.
\]
\end{theorem}
\begin{proof}
It is sufficient to show that $\hypclass$ is uniformly convergent under these
conditions. Fix $\epsilon, \delta > 0$. Let $\dist$ be a distribution on $Z$. We
wish to show that there exists $m \in \Nat$ such that when a sample~$S$ from~$Z$
is picked i.i.d according to~$\dist$
\[
    \Prtwo{S \sim \dist^m}{S \colon \forall \ h \in \hypclass,
             |L_S(h) - L_{\dist}(h)| \leq \epsilon} \geq 1 - \delta.
\]

This probability is equivalent to saying that
\[
    \Prtwo{S \sim \dist^m}{S \colon \exists \ h \in \hypclass,
             |L_S(h) - L_{\dist}(h)| > \epsilon} < \delta.
\]
Moreover, we can upper bound this probability using the Union Bound as follows:
\begin{equation} \label{eqn:union_bound}
    \Prtwo{S \sim \dist^m}{S \colon \exists \ h \in \hypclass,
             |L_S(h) - L_{\dist}(h)| > \epsilon}
    \leq \sum_{h \in \hypclass} \Prtwo{S \sim \dist^m}{S \colon |L_S(h) - L_{\dist}(h)| > \epsilon}.
\end{equation}
Note that we are using the fact that $\hypclass$ is finite here.

Recall that $L_{\dist} (h) = \Exptwo{z \sim \dist}{l(h, z)}$ and that
$L_S (h) = \frac{1}{m} \sum_{i = 1}^m l(h, z_i)$. Since the points $z_i$ are sampled
i.i.d according to~$\dist$, the random variables $l(h, z_i)$ have expectation
$L_{\dist}(h)$. The expectation of $L_S(h)$ by the linearity of expectation is also
$L_{\dist}(h)$. Thus the quantity $|L_S(h) - L_{\dist}(h)|$ is the deviation of a
random variable $L_S(h)$ from its expected value. By the Law of Large Numbers, as
$m \to \infty$, $|L_S(h) - L_{\dist}(h)| \to 0$. Thus the intuition why large-enough
samples are $\epsilon$-representative follows directly from this. However
the Law of Large Numbers is an asymptotic result and in order to be able
to provide a bound on the deviation between an empirically estimated error and its
true value for a finite sample size, we need a measure concentration inequality.

\begin{lemma}[Hoeffding's Inequality]
Let $\theta_1, \ldots, \theta_m$ be a sequence of i.i.d random variables. Suppose that
for all~$i$, $\Expone{\theta_i} = \mu$ and $\Prone{a \leq \theta_i \leq b} = 1$. Then
for any $\epsilon > 0$
\[
    \Prone{\left | \frac{1}{m} \sum_{i = 1}^m \theta_i - \mu \right | > \epsilon}
    \leq 2 \exp{\left \{ - \frac{2 m \epsilon^2}{(b - a)^2} \right \}}.
\]
\end{lemma}

Applying this to our case, we let $\theta_i = l(h, z_i)$ and $\mu = L_{\dist}(h)$.
We also assumed that the loss function values lie in the interval $[a, b]$. Then
by Hoeffding's Inequality, we have that
\begin{equation}
    \Prtwo{S \sim \dist^m}{S \colon |L_S(h) - L_{\dist}(h)| > \epsilon}
        \leq
    2 \exp{\left \{ - \frac{2 m \epsilon^2}{(b - a)^2} \right \}}.
\end{equation}
Plugging this in equation~(\ref{eqn:union_bound}), we obtain:
\begin{equation}
    \Prtwo{S \sim \dist^m}{S \colon \exists h \in \hypclass, |L_S(h) - L_{\dist}(h)| > \epsilon}
        \leq
    2 \cdot |\hypclass| \cdot \exp{\left \{ - \frac{2 m \epsilon^2}{(b - a)^2} \right \}}.
\end{equation}

Choose $m$ sufficiently large so that
\[
    2 \cdot |\hypclass| \cdot \exp{\left \{ - \frac{2 m \epsilon^2}{(b - a)^2} \right \}} \leq \delta.
\]
Simplifying, this gives us the bound:
\[
    \frac{(b - a)^2}{2 \epsilon^2} \log \left ( \frac{2 \cdot |\hypclass|}{\delta}\right ) \leq m.
\]
\end{proof}

\section*{Exercise 4.1}

We first show that $(1) \Rightarrow (2)$. For each $n \in \Nat$, define
$\epsilon_n = 1 / 2^n$ and $\delta_n = 1 / 2^n$. Then by $(1)$, for each
$n \in \Nat$, there exists $m(\epsilon_n, \delta_n)$ such that
$\forall m \geq m(\epsilon_n, \delta_n)$,
\[
    \Prtwo{S \sim \dist^{m}}{L_{\dist} (h_S) > \epsilon_n} < \delta_n.
\]
We can then upper bound $\Exptwo{S \sim \dist^m}{L_{\dist}(h_s)}$ as follows:
\begin{align*}
\Exptwo{S \sim \dist^m}{L_{\dist} (h_s)}
& \leq \epsilon_n \cdot  \Prtwo{S \sim \dist^{m}}{L_{\dist} (h_S) \leq \epsilon_n} +
    (1 - \epsilon_n) \cdot  \Prtwo{S \sim \dist^{m}}{L_{\dist} (h_S) > \epsilon_n} \\
& \leq \epsilon_n \cdot (1 - \delta_n) + (1 - \epsilon_n) \cdot \delta_n \\
& \leq \frac{1}{2^{n - 1}} - \frac{1}{2^{2n - 1}}.
\end{align*}
The first inequality follows from the fact that the loss function is from
$\hypclass \times Z \rightarrow [0, 1]$, which allows us to upper bound the value of the error
when $L_{\dist} (h_S) > \epsilon_n$ by $1 - \epsilon_n$. As $n \to \infty$, $m \to \infty$
and $\Exptwo{S \sim \dist^m}{L_{\dist} (h_s)} \to 0$, proving that $(2)$ follows.

We next show that $(2) \Rightarrow (1)$. Fix $\epsilon, \delta > 0$. Define
$\delta' = \epsilon \cdot \delta$. Since
$$\lim_{m \to \infty} \Exptwo{S \sim \dist^m}{L_{\dist}(h_s)} = 0,$$
there exists $m_1(\delta')$ such that for all $m
\geq m_1(\delta')$ we have $\Exptwo{S \sim \dist^m}{L_{\dist}(h_s)} < \delta'$.
We now lower bound $\Exptwo{S \sim \dist^m}{L_{\dist}(h_s)}$ as follows:
\begin{align*}
\Exptwo{S \sim \dist^m}{L_{\dist}(h_s)}
& = \int_{0}^{1} x \cdot  \Prtwo{S \sim \dist^{m}}{L_{\dist} (h_S) = x} \dx x \\
& \geq \int_{\epsilon}^1 x \cdot  \Prtwo{S \sim \dist^{m}}{L_{\dist} (h_S) = x} \dx x \\
& \geq \epsilon \cdot \int_{\epsilon}^1 \Prtwo{S \sim \dist^{m}}{L_{\dist} (h_S) = x} \dx x \\
& = \epsilon \cdot  \Prtwo{S \sim \dist^{m}}{L_{\dist} (h_S) \geq \epsilon}.
\end{align*}

Choose $m(\epsilon, \delta) := m_1(\epsilon \cdot \delta)$. Then for all $m
\geq m(\epsilon, \delta)$, we have that $\Exptwo{S \sim
\dist^m}{L_{\dist}(h_s)} < \epsilon \cdot \delta$, from which it follows that:
\[
    \epsilon \cdot  \Prtwo{S \sim \dist^{m}}{L_{\dist} (h_S) \geq \epsilon} <
        \epsilon \cdot \delta. \]
Condition $(1)$ follows from this.
