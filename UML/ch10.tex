\chapter{Boosting}

\section{Notes}

The Fundamental Theorem of Statistical Learning guarantees that if a hypothesis
class has finite VC-dimension~$d$, the ERM algorithm will be able to
learn it with a sample of size $\Omega(\frac{d + \log (1 / \delta)}{\epsilon})$
in the realizable setting, and $\Omega(\frac{d + \log (1 /
\delta)}{\epsilon^2})$ in the agnostic setting. From the statistical
perspective, there is \emph{no} difference between the realizable and agnostic
setting. Learning is solely determined by the VC-dimension of the hypothesis
class.

However the computational complexity of implementing the ERM algorithm varies
widely between these two settings. Implementing the ERM algorithm for learning
Boolean conjunctions or the class of axis-aligned rectangles can be efficiently
done (as in polynomial time in the input size) in the realizable case; however,
these problems are NP-hard in the agnostic case. Since the notion of
PAC-learning (in the realizable setting) deals with being able to approximate
the true hypothesis with arbitrary accuracy, it makes sense to ask when
learning is computationally feasible if we drop this requirement and consider
classifiers that are just slightly better than making a random guess.  This
leads us to the notion of $\gamma$-weak-learnability.

\begin{definition}[$\gamma$-weak-learnability]
A learning algorithm $A$ is a $\gamma$-weak-learner for a hypothesis class 
$\hypclass$ if there exists a function $\samplecomp \colon (0, 1) \rightarrow \Nat$
such that for every $\delta \in (0, 1)$, for every distribution $\dist$ over $\dom$
and every labeling function $f \colon \dom \rightarrow \{\pm 1\}$, if $A$ 
is presented with $m \geq \samplecomp (\delta)$ examples chosen i.i.d.\ 
according to $\dist$, it outputs a hypothesis that with probability at least 
$1 - \delta$ has a true error of at most $1/2 - \gamma$. 
\end{definition}

\section*{Exercise 10.1}

For $\epsilon, \delta \in (0, 1)$, define 
\[
\SampleComp{\epsilon}{\delta} := k \cdot \samplecomp(\epsilon / 2) + 
\left \lceil \frac{2 \log (4 k /  \delta)}{\epsilon^2}\right \rceil,
\]
where $k = \lceil \log (\delta / 2) / \log (\delta_0) \rceil$. Now
given a sample of size at least $\SampleComp{\epsilon}{\delta}$, divide
it into $k + 1$ chunks $S_1, \ldots, S_{k + 1}$ such that the first $k$
chunks has size $\samplecomp(\epsilon / 2)$. 

Using algorithm $A$, train the first $k$ chunks to obtain hypotheses
$\hat{h}_1, \ldots, \hat{h}_k$. Then with probability at least 
$1 - \delta_0^k \geq 1 - \delta / 2$, we have that 
\[
    \min_{1 \leq i \leq k} \{L_{\dist} (\hat{h}_i)\} 
        \leq \min_{h' \in \hypclass} \{ L_{\dist}(h') \} + \epsilon / 2.
\]
Call this event 1.

Now that we have a finite hypothesis class $\{ \hat{h}_1, \ldots, \hat{h}_k \}$,
and since the $(k + 1)$st chunk has size at least 
$\lceil 2 \log (4 k / \delta) /  \epsilon^2 \rceil$, by Corollary~4.6,
we can use ERM to obtain a hypothesis $\hat{h}$ such that 
with probability at least $1 - \delta / 2$, we have that 
\[
    L_{\dist}(\hat{h}) \leq \min_{1 \leq i \leq k} \{L_{\dist} (\hat{h}_i)\} + 
        \epsilon / 2
         \leq \min_{h' \in \hypclass} \{ L_{\dist}(h') \} + \epsilon.
\]
Call this event 2.

Using the union bound, the probability that either event~1 does \emph{not} 
happen or event~2 does \emph{not} happen is at most~$\delta$. Hence the 
probability that \emph{both} events do happen is at least $1 - \delta$. 
Hence with probability $1 - \delta$, one can find a hypothesis $\hat{h}$ 
such that $L_{\dist}(\hat{h}) \leq 
\min_{h' \in \hypclass} \{ L_{\dist}(h') \} + \epsilon$.

\section*{Exercise 10.2}

Let $\theta_0, \theta_1, \ldots, \theta_{T} \in \R$ such that 
$\theta_0 = -\infty$ and $\theta_{T} = + \infty$. Define 
$g \colon \R \rightarrow \{\pm 1\}$ such that $g(x) = (-1)^t$ 
when $x \in (\theta_{t - 1}, \theta_t]$ for $1 \leq t \leq T - 1$ 
and $g(x) = (-1)^{T}$ for $x > \theta_{T - 1}$. Define
\[
    h(x) =  \sign \left ( \sum_{t = 0}^T w_t \sign (x - \theta_{t} )\right )
\]
where $w_0 = 0.5$ and $w_t = (-1)^{t + 1}$ for $1 \leq t \leq T$. We will show 
that $h = g$ by inducting on $T$.

Let $T = 1$. Then for all $x \in \R$, $g(x) = -1$ and   
\[
h(x) = \sign(0.5 \cdot \sign(x - \theta_0) + \sign(x - \theta_1)) 
     = \sign(0.5 - 1) = -1.
\]
Let us assume that the result holds when $T = k \geq 1$. 
Consider the case when $T = k + 1$. 
We distinguish two cases here: when $k$ is even and when $k$ is odd. 
\begin{multline*}
    h(x) =  \sign \left ( 0.5 \cdot \sign (x - \theta_{0}) 
            + \sign(x - \theta_1) 
            + \cdots \right . \\ 
            + \left . (-1)^{k + 1} \cdot \sign(x - \theta_k) 
            + (-1)^{k + 2} \cdot \sign(x - \theta_{k + 1}) \right )
\end{multline*}
First assume that $k$ is even. By induction hypothesis, we know that 
for all $x < \theta_{k}$, $h(x) = g(x)$. For $x = \theta_k$, $g(x) = +1$
and 
\begin{multline*}
h(x) = \sign( 0.5 + (1 - 1) + \ldots + (1 + (-1)^{k + 1} \cdot \sign(x - \theta_k)) + \\ 
       (-1)^{k + 2} \sign(x - \theta_{k + 1})). 
\end{multline*}
This equals $\sign(0.5 + 2 - 1) = +1$. 
For $x > \theta_{k}$, $g(x) = -1$ and 
\begin{align*}
    h(x) & = \sign(0.5 + (1 - 1) + \ldots + (1 - 1) + (-1)^{k + 2} 
             \sign(x - \theta_{k + 1})) \\
         & = \sign(0.5 - 1) \\
         & = -1.
\end{align*}
One can show that $h = g$ for $k$ odd. 
