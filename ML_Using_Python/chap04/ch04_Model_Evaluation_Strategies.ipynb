{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "Model evaluation is done by splitting the available data into three sets: training data, validation data and test data. Training data is used to train the model; validation data is used to find the best configuration parameters (hyperparameters) of the model; and the test data is used to evaluate the generalizability of the model.\n",
    "\n",
    "We consider three classic evaluation recipes: \n",
    "\n",
    "* simple hold-out validation\n",
    "* $K$-fold validation, and \n",
    "* iterated $K$-fold validation with shuffling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Hold-Out Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def train_val_split(data: np.array, val_set_frac: float) -> Tuple[np.array, np.array]:\n",
    "    assert val_set_frac >= 0 and val_set_frac <= 1\n",
    "    \n",
    "    local_cpy = np.copy(data)\n",
    "    np.random.shuffle(local_cpy)\n",
    "    \n",
    "    idx = int(len(local_cpy) * val_set_frac)\n",
    "    val_data = local_cpy[: idx]\n",
    "    train_data = local_cpy[idx :]\n",
    "    \n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One tests a set of models with different configurations (hyperparameters) and chooses the best configuration.\n",
    "\n",
    "```python\n",
    "scores = []\n",
    "for model_config in list_of_configurations:\n",
    "    model = get_model(model_config)\n",
    "    train, val = train_val_split(training_data)\n",
    "    model.fit(train)\n",
    "    scores.append(model.evaluate(val))\n",
    "```\n",
    "\n",
    "Then the chosen model is evaluated with the test data.\n",
    "```python\n",
    "model = get_model(chosen_config)                                        \n",
    "model.train(training_data)             \n",
    "test_score = model.evaluate(test_data) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $K$-Fold Cross Validation\n",
    "\n",
    "Partition the dataset into $K$ sets of roughly equal size. For $1 \\leq i \\leq K$, do the following: train on $K - 1$ partitions and evaluate on the $i$th partition and keep a track of the score. The final score of the model is the average of the scores in each of these iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "def k_fold_crossvalidation(data: np.array, k: int, model_configuration: Dict[Any, Any]) -> float:\n",
    "    assert k > 1\n",
    "    \n",
    "    num_validation_samples = len(data) // k\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    validation_scores = []\n",
    "    \n",
    "    for fold in range(k):\n",
    "        validation_data = data[num_validation_samples * fold: num_validation_samples * (fold + 1)] \n",
    "        training_data = np.concatenate(data[:num_validation_samples * fold], \n",
    "                                       data[num_validation_samples * (fold + 1):], axis=0)\n",
    "        model = get_model(model_configuration)\n",
    "        model.train(training_data)\n",
    "        validation_score = model.evaluate(validation_data)\n",
    "        validation_scores.append(validation_score)\n",
    "\n",
    "    return np.average(validation_scores)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One we have evaluated models with different configurations, we select the configuration that is best and then evaluate the model on the test data.\n",
    "\n",
    "```python\n",
    "model = get_model(final_configuration)                                                   \n",
    "model.train(data)                                                     \n",
    "test_score = model.evaluate(test_data)  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "One cannot directly feed raw data to a neural network in most situations. Raw data needs to be processed and typical procesing steps include _vectorization_, _normalization_, _handling missing values_ and _feature extraction_. \n",
    "\n",
    "### Data Vectorization\n",
    "\n",
    "All inputs to a neural network must be in the form of tensors of floating point data. Whatever the raw data, be it text, sound or images, must be first transformed into tensors. \n",
    "\n",
    "### Data Normalization\n",
    "\n",
    "In general, it is not a good idea to feed a neural network numbers that are very large compared to the initial weights with which the network is initialized or data that is very heterogeneous (data where different features vary over widely different ranges). Large numbers can cause the output of the neurons to take on very large or very small values. This, in turn, makes it difficult to update the gradient of the activation function of those neurons leading to slow convergence.\n",
    "\n",
    "Simple rule of thumb:\n",
    "\n",
    "* Values should be in the 0-1 range\n",
    "* Features should be homogeneous in the sense of having values in the same range\n",
    "\n",
    "Simple normalization practice\n",
    "\n",
    "* Normalize each feature independently to have a mean of $0$\n",
    "* Normalize each feature independently to have a standard deviation of $1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Classification for Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 3s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/somnath/install/anaconda3/envs/dl/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/somnath/install/anaconda3/envs/dl/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "# num_words = 10000 keeps the 10000 most frequently occurring words\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([max(s) for s in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# decoding labels to words\n",
    "\n",
    "word_index = imdb.get_word_index()                 \n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])            \n",
    "\n",
    "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))        \n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.                          \n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)                  \n",
    "x_test = vectorize_sequences(test_data)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
