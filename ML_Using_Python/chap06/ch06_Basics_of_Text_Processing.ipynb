{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Definitions\n",
    "\n",
    "Deep learning for natural language processing is pattern recognition applied to textual data. Textual data has to be first processed before one can apply machine learning techniques. Typically, the text is broken down either into characters or words or $n$-grams. An $n$-gram is a consecutive sequence of $n$ or fewer characters or words that can be extracted from a sentence. The units into which textual data is broken down (either characters or words or $n$-grams) are called _tokens_. The process in which text is broken down into tokens is called _tokenization_ and the tokens are assigned numeric vectors in accordance with some embedding scheme. The major token embedding schemes are:\n",
    "* one hot encoding of tokens\n",
    "* token embedding (this is applied to words and is called word embedding)\n",
    "\n",
    "### One Hot Encoding\n",
    "\n",
    "This consists of assigning a unique integer index to every word from a finite vocabulary of size $N$. The vector associated with the $i$th word from this vocabulary is a bit vector that has a $1$ in position $i$ and zeros elsewhere. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def tokenize(sample: str, drop_singletons=False):\n",
    "    neither_char_nor_number = '[^A-Za-z0-9]+'\n",
    "    without_special_chars = re.sub(f'{neither_char_nor_number}', ' ', sample)\n",
    "    no_leading_trailing_spaces = without_special_chars.strip() \n",
    "    words = no_leading_trailing_spaces.split(' ')\n",
    "    \n",
    "    if drop_singletons:\n",
    "        words = [w for w in words if len(w) > 1]\n",
    "        \n",
    "    return words\n",
    "\n",
    "def create_token_dict(samples: List[str]):\n",
    "    token_dict = dict()\n",
    "    for sample in samples:\n",
    "        words = tokenize(sample, drop_singletons=True)\n",
    "        for w in words:\n",
    "            if w not in token_dict.keys():\n",
    "                token_dict[w] = len(token_dict) + 1\n",
    "    return token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Consider': 1,\n",
       " 'the': 2,\n",
       " 'number': 3,\n",
       " '90': 4,\n",
       " 'This': 5,\n",
       " 'large': 6,\n",
       " 'fine': 7,\n",
       " 'with': 8,\n",
       " 'smaller': 9,\n",
       " 'ones': 10}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_token_dict([\"Consider the number  90**5.\", \"This a large number!\", \"I'm fine with smaller ones:)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "\n",
    "# Builds the word index\n",
    "tokenizer.fit_on_texts(samples)\n",
    "\n",
    "# Strings into lists of integer indices\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
