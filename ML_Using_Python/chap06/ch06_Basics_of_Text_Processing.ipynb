{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Definitions\n",
    "\n",
    "Deep learning for natural language processing is pattern recognition applied to textual data. Textual data has to be first processed before one can apply machine learning techniques. Typically, the text is broken down either into characters or words or $n$-grams. An $n$-gram is a consecutive sequence of $n$ or fewer characters or words that can be extracted from a sentence. The units into which textual data is broken down (either characters or words or $n$-grams) are called _tokens_. The process in which text is broken down into tokens is called _tokenization_ and the tokens are assigned numeric vectors in accordance with some embedding scheme. The major token embedding schemes are:\n",
    "* one hot encoding of tokens\n",
    "* token embedding (this is applied to words and is called word embedding)\n",
    "\n",
    "## One Hot Encoding\n",
    "\n",
    "This consists of assigning a unique integer index to every word from a finite vocabulary of size $N$. The vector associated with the $i$th word from this vocabulary is a bit vector that has a $1$ in position $i$ and zeros elsewhere. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def tokenize(sample: str, drop_singletons=False):\n",
    "    neither_char_nor_number = '[^A-Za-z0-9]+'\n",
    "    without_special_chars = re.sub(f'{neither_char_nor_number}', ' ', sample)\n",
    "    no_leading_trailing_spaces = without_special_chars.strip() \n",
    "    words = no_leading_trailing_spaces.split(' ')\n",
    "    \n",
    "    if drop_singletons:\n",
    "        words = [w for w in words if len(w) > 1]\n",
    "        \n",
    "    return words\n",
    "\n",
    "def create_token_dict(samples: List[str]):\n",
    "    token_dict = dict()\n",
    "    for sample in samples:\n",
    "        words = tokenize(sample, drop_singletons=True)\n",
    "        for w in words:\n",
    "            if w not in token_dict.keys():\n",
    "                token_dict[w] = len(token_dict) + 1\n",
    "    return token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Consider': 1,\n",
       " 'the': 2,\n",
       " 'number': 3,\n",
       " '90': 4,\n",
       " 'This': 5,\n",
       " 'large': 6,\n",
       " 'fine': 7,\n",
       " 'with': 8,\n",
       " 'smaller': 9,\n",
       " 'ones': 10}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_token_dict([\"Consider the number  90**5.\", \"This a large number!\", \"I'm fine with smaller ones:)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "\n",
    "# Builds the word index\n",
    "tokenizer.fit_on_texts(samples)\n",
    "\n",
    "# Strings into lists of integer indices\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Encodings\n",
    "\n",
    "A word encoding is a map from a vocabulary $V$ to $\\mathbf{R}^n$, where typically $n \\ll |V|$. They give an efficient, dense representation where similar words have encodings that are close to each other. There is no universally \"good\" embedding: an embedding that is suitable for movie reviews classification may not be suitable for classifying scientific documents as the relative importance of word pairs in these two fields differ. Thus it is imperative to learn a new word embedding for each new task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "# number of possible tokens = 1000 = 1 + maximum word index; dimensionality = 64\n",
    "embedding_layer = Embedding(1000, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/somnath/install/anaconda3/envs/dl/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/somnath/install/anaconda3/envs/dl/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "# Loading the IMDB data for use with the embedding layer\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "# we will choose the top 10000 most commonly used words; so word index = [0 ... 9999]\n",
    "max_features = 10000\n",
    "# Each review would be truncated to the first 20 words\n",
    "maxlen = 20\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Embedding Layer\n",
    "\n",
    "\n",
    "* input_dim: Integer. Size of the vocabulary, i.e. maximum integer index + 1.\n",
    "* output_dim: Integer. Dimension of the dense embedding.\n",
    "* input_length: Length of input sequences, when it is constant. This argument is required if you are going to connect Flatten then Dense layers upstream (without it, the shape of the dense outputs cannot be computed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "# input_dim = size of the input vocabulary\n",
    "# output_dim = size of the embedded vectors\n",
    "# input_length = size of each input sequence\n",
    "model.add(Embedding(input_dim=max_features, output_dim=8, input_length=maxlen))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.6659 - acc: 0.6296 - val_loss: 0.6105 - val_acc: 0.7040\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.5338 - acc: 0.7534 - val_loss: 0.5220 - val_acc: 0.7296\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.4575 - acc: 0.7881 - val_loss: 0.4982 - val_acc: 0.7476\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.4203 - acc: 0.8081 - val_loss: 0.4929 - val_acc: 0.7530\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3959 - acc: 0.8223 - val_loss: 0.4930 - val_acc: 0.7548\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.3757 - acc: 0.8324 - val_loss: 0.4960 - val_acc: 0.7578\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3577 - acc: 0.8439 - val_loss: 0.5013 - val_acc: 0.7592\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3408 - acc: 0.8532 - val_loss: 0.5059 - val_acc: 0.7604\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3237 - acc: 0.8626 - val_loss: 0.5121 - val_acc: 0.7546\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.3067 - acc: 0.8726 - val_loss: 0.5204 - val_acc: 0.7534\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.embeddings.Embedding at 0x7fde3c4d4be0>,\n",
       " <tensorflow.python.keras.layers.core.Flatten at 0x7fde3c51c550>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fde3c4de460>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06403131,  0.02865895,  0.01554053, ...,  0.08405918,\n",
       "        -0.07743204,  0.06030514],\n",
       "       [-0.05036338,  0.05972435, -0.01678254, ...,  0.01711121,\n",
       "        -0.09139732, -0.06354951],\n",
       "       [ 0.03289519, -0.02895788,  0.10627425, ...,  0.0610166 ,\n",
       "        -0.02239222,  0.0299639 ],\n",
       "       ...,\n",
       "       [ 0.01399802,  0.00061152,  0.01824389, ...,  0.02553368,\n",
       "        -0.04194051, -0.02597152],\n",
       "       [-0.03236458, -0.00690295, -0.0175093 , ..., -0.00018737,\n",
       "        -0.02844385,  0.00599523],\n",
       "       [-0.02778889, -0.04072574,  0.00177168, ..., -0.00709735,\n",
       "        -0.00107574, -0.03337316]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06403131,  0.02865895,  0.01554053,  0.08978625, -0.00389895,\n",
       "        0.08405918, -0.07743204,  0.06030514], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].get_weights()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].get_weights()[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
