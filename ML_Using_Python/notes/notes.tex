\documentclass[12pt]{article}

%-------------------------------------------------------
% THEMES, COLORS
%-------------------------------------------------------
\usepackage[top=2.5cm, bottom=3cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{mathtools,bm,amsthm}
\usepackage{underscore}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{algpseudocode}

%--------------------------------------------------------
% COLOR DEFINITIONS
%---------------------------------------------------------
\definecolor{amber}{rgb}{1.0, 0.49, 0.0}
\definecolor{olivedrab}{rgb}{0.42, 0.56, 0.14}
\definecolor{darkolivegreen}{rgb}{0.33, 0.42, 0.18}
\definecolor{chromeyellow}{rgb}{1.0, 0.65, 0.0}
\definecolor{beige}{rgb}{0.96, 0.96, 0.86}
\definecolor{bluegreen}{rgb}{3, 166, 155}
\definecolor{pitchblack}{rgb}{0, 0, 0}
\definecolor{lightbeige}{rgb}{255, 251, 241}
\definecolor{mediumgray}{rgb}{183, 183, 183}
\definecolor{carrotorange}{rgb}{0.93, 0.57, 0.13}
\definecolor{orange-red}{rgb}{1.0, 0.27, 0.0}
\definecolor{ferngreen}{rgb}{0.31, 0.47, 0.26}
\definecolor{darkspringgreen}{rgb}{0.09, 0.45, 0.27}
\definecolor{cocoabrown}{rgb}{0.82, 0.41, 0.12}
\definecolor{darkorange}{rgb}{1.0, 0.55, 0.0}
\definecolor{deepcarrotorange}{rgb}{0.91, 0.41, 0.17}
%---------------------------------------------------------

%-------------------------------------------------------------------------
% NEW THEME
%-------------------------------------------------------------------------
\definecolor{UBCblue}{rgb}{0.04706, 0.13725, 0.26667} % UBC Blue (primary)
\definecolor{UBCgrey}{rgb}{0.3686, 0.5255, 0.6235} % UBC Grey (secondary)

%-------------------------------------------------------
% DEFFINING AND REDEFINING COMMANDS
%-------------------------------------------------------
\newcommand\var{\texttt}
\renewcommand{\emptyset}{\varnothing}
%-----------------------------------------------------------------------------
% FONTS TRIED
%-----------------------------------------------------------------------------
%\usepackage{ccfonts}
%\usepackage{mathpazo}
%\usepackage{newtxtext}
%\usepackage{newtxmath}
%% Times New Roman
%\setromanfont[
%BoldFont=timesbd.ttf,
%ItalicFont=timesi.ttf,
%BoldItalicFont=timesbi.ttf,
%]{times.ttf}
%% Arial
%\setsansfont[
%BoldFont=arialbd.ttf,
%ItalicFont=ariali.ttf,
%BoldItalicFont=arialbi.ttf
%]{arial.ttf}
%% Courier New
%\setmonofont[Scale=0.90,
%BoldFont=courbd.ttf,
%ItalicFont=couri.ttf,
%BoldItalicFont=courbi.ttf
%]{cour.ttf}
%-----------------------------------------------------------------------------
% FONT PACKAGES
%-----------------------------------------------------------------------------
\usepackage{fontspec}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[bitstream-charter]{mathdesign}
\setmainfont{XCharter}

%-------------------------------------------------------
% INFORMATION IN THE TITLE PAGE
%-------------------------------------------------------


\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem*{note}{Note}
\newcommand{\dist}{\ensuremath{\mathcal{D}}}
\newcommand{\R}{\ensuremath{\bm{R}}}
\newcommand{\Rone}{\ensuremath{\bm{R}}}
\newcommand{\Rpos}{\ensuremath{\bm{R}_{+}}}
\newcommand{\Rtwo}{\ensuremath{\bm{R}^2}}
\newcommand{\Prtwo}[2]{\bm{Pr}_{#1} \left \{ #2 \right \}}
\newcommand{\Prone}[1]{\bm{Pr} \left \{ #1 \right \}}
\newcommand{\Exptwo}[2]{\bm{E}_{#1} \left [ #2 \right ]}
\newcommand{\Expone}[1]{\bm{E} \left [ #1 \right ]}
\newcommand{\ceiling}[1]{\ensuremath{\left \lceil #1 \right \rceil}}
\newcommand{\angular}[1]{\ensuremath{\left \langle #1 \right \rangle}}
\newcommand{\floor}[1]{\ensuremath{\left \lfloor #1 \right \rfloor}}
\newcommand{\dx}{\ensuremath{\text{d}}}
\newcommand{\ind}{\ensuremath{\bm{1}}}
\newcommand{\Mod}[1]{\ (\bm{mod}\ #1)}
\newcommand{\basisvec}{\ensuremath{\boldsymbol{e}}}
\newcommand{\vect}[1]{\ensuremath{\boldsymbol{#1}}}
\newcommand{\conj}[1]{\ensuremath{\bar{#1}}}
\newcommand{\eigenspace}[2]{\ensuremath{\mathscr{E}_{#1}(#2)}}
\newcommand{\linspace}[1]{\ensuremath{\mathscr{L}(#1)}}
\newcommand{\innerprod}[2]{\ensuremath{\left \langle #1, #2 \right \rangle}}
\newcommand{\adj}[1]{\ensuremath{#1^{\star}}}
\newcommand{\inv}[1]{\ensuremath{#1^{-1}}}

\DeclareMathOperator{\ERM}{ERM}
\DeclareMathOperator{\pos}{POS}
\DeclareMathOperator{\HS}{HS}
\DeclareMathOperator{\HHS}{HHS}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\image}{Im}
\DeclareMathOperator{\kernel}{Ker}
\DeclareMathOperator{\nullspace}{null}
\DeclareMathOperator{\range}{range}
\DeclareMathOperator{\diag}{diag}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\newcommand{\trans}[1]{\ensuremath{#1}^{\intercal}}

% LSTM operator names
\DeclareMathOperator{\rem}{rem}
\DeclareMathOperator{\save}{save}
\DeclareMathOperator{\focus}{focus}
\DeclareMathOperator{\ltm}{ltm}
\DeclareMathOperator{\wm}{wm}

%\newtheorem{example}{Example}[chapter]

\newtheorem{exercise}{Exercise}

\newenvironment{solution}
  {\renewcommand\qedsymbol{$\blacksquare$}\begin{proof}[Solution]}
  {\end{proof}}

\title{Random Notes}

\author{Somnath Sikdar}
\date{\today}

\begin{document}
\section{LSTMs}
LSTMs were developed in order to circumvent the vanishing gradient problem that 
plagues multi-layered RNNs. LSTMs are equipped with a long-term memory and a 
short-term working memory. 

Let $x_t \in \R^{p}$ denote the input at time $t$; let $\ltm_{t} \in \R^{d}$ 
and $\wm_{t} \in \R^{d}$ denote, respectively, the long-term memory and the 
working memory available to the LSTM cell at time $t$. 

\medskip

\noindent \textbf{Updating the long-term memory.} In order to update the long-term
memory at step $t$, the LSTM first figures out what to remember from the 
long-term memory of the last step $t - 1$. 
\begin{equation}
\rem_{t} = \sigma \left ( W_r \cdot x_t + U_r \cdot \wm_{t - 1} + b_r \right ). 
\end{equation}
This is accomplished using a one-layer neural network with a sigmoid activation 
function that estimates the weight matrices $W_r \in \R^{p \times d}$, 
$U_r \in \R^{d \times d}$ and the bias vector $b_r \in \R^{d}$. Since the 
activation function is sigmoid, the components of $\rem_t$ are between $0$ and 
$1$. If a component is closer to $1$, we would want to remember it; if it is 
close to $0$, then we want to forget it. 

It next calculates a ``candidate'' vector to add to its long-term memory. This 
is done using a single-layer neural network with a $\tanh$ activation function.
Denote this candidate by $\ltm_{t}'$.
\begin{equation}
\ltm_{t}' = \sigma \left ( W_l \cdot x_t + U_l \cdot \wm_{t - 1} + b_l \right ). 
\end{equation} 
As usual, $W_l \in \R^{p \times d}$, $U_l \in \R^{d \times d}$ and $b_l \in \R^{d}$. 

Not all parts of this candidate vector may be worth remembering. As such, a 
$\save_{t}$ vector is created using another single-layer neural network with a 
sigmoid activation function. 
\begin{equation}
\save_{t} = \sigma \left ( W_s \cdot x_t + U_s \cdot \wm_{t - 1} + b_s \right ). 
\end{equation} 
The dimensions of the weight matrices $W_s$, $U_s$ and the bias vector $b_s$ 
are such that $\save_{t} \in \R^{d}$.
Now the long-term component of the cell is computed using:
\begin{equation}
\ltm_{t} = \rem_{t} \odot \ltm_{t - 1} + \save_{t} \odot \ltm_{t}',
\end{equation} 
where $\odot$ represents component-wise multiplication of the $d$-dimensional 
vectors.

\medskip
\noindent \textbf{Updating the working memory.} To do this, the LSTM first 
calculates what parts of the long-term memory it currently wants to focus on. 
It uses another single-layer neural network with a sigmoid activation to 
calculate $\focus_{t} \in \R^{d}$. 
\begin{equation}
\focus_{t} = \sigma \left ( W_f \cdot x_t + U_f \cdot \wm_{t - 1} + b_f \right ). 
\end{equation} 
It then updates its working memory using:
\begin{equation}
\wm_{t} = \focus_{t} \odot \tanh(\ltm_{t}).
\end{equation} 
\end{document}

