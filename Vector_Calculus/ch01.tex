\chapter{Linear Algebra Basics}

\section{Linear Functions}

A function $L \colon \R^m \rightarrow \R^n$ is a linear function if for all
$\vect{x}, \vect{y} \in \R^m$ and for all $a, b \in \R$ 
\[
    L(a \vect{x} + b \vect{y}) = a L(\vect{x}) + b L(\vect{y}).
\]
It follows (by induction) that for all $\vect{x}_1, \ldots, \vect{x}_r \in \R^m$
and all $a_1, \ldots, a_r \in \R$
\[
    L(a_1 \vect{x}_1 + \cdots + a_r \vect{x}_r) = 
        a_1 L(\vect{x}_1) + \cdots + a_r L(\vect{x}_r).
\]

\begin{theorem}
A linear function $L \colon \R^m \rightarrow \R^n$ is completely determined by 
its effect on the standard basis vectors $\vect{e}_1, \ldots, \vect{e}_m$ of $\R^m$.
An arbitrary choice of vectors $L(\vect{e}_1), \ldots, L(\vect{e}_m)$ of $\R^n$ 
determines a linear function from $\R^m$ to $\R^n$.
\end{theorem}
\begin{proof}
Given any vector $\vect{x} \in \R^m$, we can express it as a unique linear
combination~$ \sum_{i = 1}^m \alpha_i \vect{e}_i$ of the basis vectors.  By the
linearity of $L$, $L(\vect{x}) = \sum_i \alpha_i L(\vect{e}_i)$ which 
is completely specified by $L(\vect{e}_1), \ldots, L(\vect{e}_m)$.

Let $\vect{b}_1, \ldots, \vect{b}_m$ be any vectors in $\R^n$. Define a 
map~$L$ from $\R^m$ to $\R^n$ as follows: for 
$\vect{x} = \sum_{i = 1}^m \alpha_i \vect{e}_i \in \R^m$, 
$L(\vect{x}) = \sum_{i = 1}^m \alpha_i \vect{b}_i$. Then 
$L(\vect{e}_i) = \vect{b}_i$ for all $1 \leq i \leq m$ and for all
$\vect{x}, \vect{y} \in \R^m$ and all $a, b \in \R$:
\[
    L(a \vect{x} + b \vect{y}) = \sum_{i = 1}^m (a x_i + b y_i) \vect{b}_i 
        = a \sum_{i} x_i \vect{b}_i + b \sum_{i} y_i \vect{b}_i 
        = a L(\vect{x}) + b L(\vect{y})
\] 
\end{proof}

Note that the domain of definition of a linear function must be a vector space. An 
non-linear function can be defined on a subset of a vector space.

\section{Image and Kernel of a Linear Function}
The image~$\image (L)$ of a linear function~$L \colon \R^m \rightarrow \R^n$ is
the set of vectors in~$\R^n$ that $L$ maps~$\R^m$ to. In symbols, $\image (L)
:= \{ L (\vect{x}) \in \R^n \colon \vect{x} \in \R^m\}$.  The kernel~$\kernel
(L)$ of~$L$ is the set of vectors in~$\R^m$ that $L$ maps to the zero vector
in~$\R^n$: $\kernel (L) := \{\vect{x} \in \R^m \colon L(\vect{x}) =
\vect{0}_n\}$.

\begin{theorem}
Let $L \colon \R^m \rightarrow \R^n$ be a linear function. Then the following
hold:
\begin{enumerate}
	\item $\image (L)$ is a subspace of $\R^n$;
	\item $\kernel (L)$ is a subspace of $\R^m$;
	\item $\dim (\image (L)) + \dim (\kernel (L)) = \dim (\R^m) = m$.
\end{enumerate}
\end{theorem}
\begin{proof}
The proof of~$(1)$ and~$(2)$ are similar. For proving~$(1)$, let 
$\vect{z}_1, \vect{z}_2 \in \image(L)$ and $\alpha, \beta \in \R$. 
Then there exist $\vect{x}_1, \vect{x}_2 \in \R^m$ such that 
\[
	L(\alpha \vect{x}_1 + \beta \vect{x}_2) = \alpha  \vect{z}_1 + \beta \vect{z}_2,
\]
implying that $\alpha  \vect{z}_1 + \beta \vect{z}_2 \in \image (L)$. Since 
$\image(L)$ is closed under vector addition and multiplication by scalars, it is 
a vector space. Since it is a subset of $\R^n$, it must be a subspace of $\R^n$.

To prove~$(3)$, let us assume that $\dim ( \kernel (L)) = k$ and that 
$\vect{b}_1, \ldots, \vect{b}_k$ is a basis of $\kernel (L)$. Extend this 
basis to a basis $\vect{b}_1, \ldots, \vect{b}_k, \vect{b}_{k + 1}, \ldots, \vect{b}_m$
for $\R^m$. Then for every vector $\vect{x} \in \R^m$ there exist scalars 
$\beta_1, \ldots, \beta_m$ such that $\vect{x} = \sum_{i = 1}^k \beta_i \vect{b}_i$. 
Moreover since $L$ is linear,
\begin{align*}
	L(\vect{x}) & = \sum_{i = 1}^{m} \beta_i L(\vect{b}_i) \\
				& = \sum_{i = k + 1}^{m} \beta_i L(\vect{b}_i).
\end{align*}
This shows that every vector in $\image (L)$ can be expressed as a linear 
combination of the vectors $L(\vect{b}_{k + 1}), \ldots, L(\vect{b}_m)$. 
To show that they form a basis of $\image (L)$, it is sufficient to show
that they are linearly independent. Suppose not. Then there exist scalars 
$\beta_{k + 1}, \ldots, \beta_m$, not all zero, such that 
$\sum_{i = k + 1}^m \beta_i L(\vect{b}_i) = \vect{0}_n$. By the linearity of~$L$, 
we have $L(\sum_{i = k + 1}^m \beta_i \vect{b}_i) = \vect{0}_n$ and hence 
$\vect{0}_m \neq \sum_{i = k + 1}^m \beta_i \vect{b}_i \in \kernel (L)$. 
This is a contradiction since the vectors $\vect{b}_{k + 1}, \ldots, \vect{b}_m$
are not in the space spanned by the vectors $\vect{b}_1, \ldots, \vect{b}_k$. 
Thus the vectors $\beta_{k + 1}, \ldots, \beta_m$ must be independent and form
a basis of $\image (L)$. This proves~$(3)$. 
\end{proof}
