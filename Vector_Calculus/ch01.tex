\chapter{Linear Algebra Basics}

\section{Linear Functions}

A function $L \colon \R^m \rightarrow \R^n$ is a linear function if for all
$\vect{x}, \vect{y} \in \R^m$ and for all $a, b \in \R$ 
\[
    L(a \vect{x} + b \vect{y}) = a L(\vect{x}) + b L(\vect{y}).
\]
It follows (by induction) that for all $\vect{x}_1, \ldots, \vect{x}_r \in \R^m$
and all $a_1, \ldots, a_r \in \R$
\[
    L(a_1 \vect{x}_1 + \cdots + a_r \vect{x}_r) = 
        a_1 L(\vect{x}_1) + \cdots + a_r L(\vect{x}_r).
\]

\begin{theorem}
A linear function $L \colon \R^m \rightarrow \R^n$ is completely determined by 
its effect on the standard basis vectors $\vect{e}_1, \ldots, \vect{e}_m$ of $\R^m$.
An arbitrary choice of vectors $L(\vect{e}_1), \ldots, L(\vect{e}_m)$ of $\R^n$ 
determines a linear function from $\R^m$ to $\R^n$.
\end{theorem}
\begin{proof}
Given any vector $\vect{x} \in \R^m$, we can express it as a unique linear
combination~$ \sum_{i = 1}^m \alpha_i \vect{e}_i$ of the basis vectors.  By the
linearity of $L$, $L(\vect{x}) = \sum_i \alpha_i L(\vect{e}_i)$ which 
is completely specified by $L(\vect{e}_1), \ldots, L(\vect{e}_m)$.

Let $\vect{b}_1, \ldots, \vect{b}_m$ be any vectors in $\R^n$. Define a 
map~$L$ from $\R^m$ to $\R^n$ as follows: for 
$\vect{x} = \sum_{i = 1}^m \alpha_i \vect{e}_i \in \R^m$, 
$L(\vect{x}) = \sum_{i = 1}^m \alpha_i \vect{b}_i$. Then 
$L(\vect{e}_i) = \vect{b}_i$ for all $1 \leq i \leq m$ and for all
$\vect{x}, \vect{y} \in \R^m$ and all $a, b \in \R$:
\[
    L(a \vect{x} + b \vect{y}) = \sum_{i = 1}^m (a x_i + b y_i) \vect{b}_i 
        = a \sum_{i} x_i \vect{b}_i + b \sum_{i} y_i \vect{b}_i 
        = a L(\vect{x}) + b L(\vect{y})
\] 
\end{proof}

Note that the domain of definition of a linear function must be a vector space. An 
non-linear function can be defined on a subset of a vector space.

\section{Image and Kernel of a Linear Function}
The image~$\image (L)$ of a linear function~$L \colon \R^m \rightarrow \R^n$ is
the set of vectors in~$\R^n$ that $L$ maps~$\R^m$ to. In symbols, $\image (L)
:= \{ L (\vect{x}) \in \R^n \colon \vect{x} \in \R^m\}$.  The kernel~$\kernel
(L)$ of~$L$ is the set of vectors in~$\R^m$ that $L$ maps to the zero vector
in~$\R^n$: $\kernel (L) := \{\vect{x} \in \R^m \colon L(\vect{x}) =
\vect{0}_n\}$.

\begin{theorem}
Let $L \colon \R^m \rightarrow \R^n$ be a linear function. Then the following
hold:
\begin{enumerate}
	\item $\image (L)$ is a subspace of $\R^n$;
	\item $\kernel (L)$ is a subspace of $\R^m$;
	\item $\dim (\image (L)) + \dim (\kernel (L)) = \dim (\R^m) = m$.
\end{enumerate}
\end{theorem}
\begin{proof}
The proof of~$(1)$ and~$(2)$ are similar. For proving~$(1)$, let 
$\vect{z}_1, \vect{z}_2 \in \image(L)$ and $\alpha, \beta \in \R$. 
Then there exist $\vect{x}_1, \vect{x}_2 \in \R^m$ such that 
\[
	L(\alpha \vect{x}_1 + \beta \vect{x}_2) = \alpha  \vect{z}_1 + \beta \vect{z}_2,
\]
implying that $\alpha  \vect{z}_1 + \beta \vect{z}_2 \in \image (L)$. Since 
$\image(L)$ is closed under vector addition and multiplication by scalars, it is 
a vector space. Since it is a subset of $\R^n$, it must be a subspace of $\R^n$.

To prove~$(3)$, let us assume that $\dim ( \kernel (L)) = k$ and that 
$\vect{b}_1, \ldots, \vect{b}_k$ is a basis of $\kernel (L)$. Extend this 
basis to a basis $\vect{b}_1, \ldots, \vect{b}_k, \vect{b}_{k + 1}, \ldots, \vect{b}_m$
for $\R^m$. Then for every vector $\vect{x} \in \R^m$ there exist scalars 
$\beta_1, \ldots, \beta_m$ such that $\vect{x} = \sum_{i = 1}^k \beta_i \vect{b}_i$. 
Moreover since $L$ is linear,
\begin{align*}
	L(\vect{x}) & = \sum_{i = 1}^{m} \beta_i L(\vect{b}_i) \\
				& = \sum_{i = k + 1}^{m} \beta_i L(\vect{b}_i).
\end{align*}
This shows that every vector in $\image (L)$ can be expressed as a linear 
combination of the vectors $L(\vect{b}_{k + 1}), \ldots, L(\vect{b}_m)$. 
To show that they form a basis of $\image (L)$, it is sufficient to show
that they are linearly independent. Suppose not. Then there exist scalars 
$\beta_{k + 1}, \ldots, \beta_m$, not all zero, such that 
$\sum_{i = k + 1}^m \beta_i L(\vect{b}_i) = \vect{0}_n$. By the linearity of~$L$, 
we have $L(\sum_{i = k + 1}^m \beta_i \vect{b}_i) = \vect{0}_n$ and hence 
$\vect{0}_m \neq \sum_{i = k + 1}^m \beta_i \vect{b}_i \in \kernel (L)$. 
This is a contradiction since the vectors $\vect{b}_{k + 1}, \ldots, \vect{b}_m$
are not in the space spanned by the vectors $\vect{b}_1, \ldots, \vect{b}_k$. 
Thus the vectors $\beta_{k + 1}, \ldots, \beta_m$ must be independent and form
a basis of $\image (L)$. This proves~$(3)$. 
\end{proof}

\section{Quadratic Forms}

A function $q \colon \R^m \rightarrow \R$ is called a \emph{quadratic form} 
if there exists a real symmetric $m \times m$ matrix $A$ such that for all 
$\vect{x} \in \R^m$ such that
\[
	q(\vect{x}) = \trans{\vect{x}} A \vect{x}.
\]
The right-hand side of the equation may be written as:
\begin{align*}
(x_1, \ldots, x_m) \begin{pmatrix} 
						a_{11} & a_{12} & \cdots & a_{m 1} \\
						\vdots & \vdots & \cdots & \vdots \\
						a_{m1} & a_{m2} & \cdots & a_{mm} 	
                   \end{pmatrix} 
				   \begin{pmatrix}
						x_1 \\
						\vdots \\
						x_m
				   \end{pmatrix} & = (x_1, \ldots, x_m) [x_1 A_{* 1} + \cdots + x_m A_{* m}] \\
								 & = \sum_{i = 1}^m \sum_{j = 1}^m x_i a_{ij} x_j.
\end{align*} 
The symbol $A_{* j}$ denotes the $j$th column of $A$. The name ``quadratic'' form arises 
from the last expression which is what a quadratic expression in $m$ variables looks like.
\begin{example}
The function 
\[
	q(h_1, h_2, h_3) = h_1^2 + h_2^2 + h_3^2 + 2h_1h_2 + 2h_2h_3 + 2h_1h_3 = (h_1 + h_2 + h_3)^2
\]
can be expressed as $\trans{\vect{h}} A \vect{h}$, where $A$ is the all-ones $3 \times 3$ matrix. 
\end{example}

\subsection{Definiteness}
A quadratic form $q \colon \R^m \rightarrow \R$ is 
\begin{enumerate}
	\item \emph{positive definite} if for all non-zero $\vect{x} \in \R^m$, $q(\vect{x}) > 0$;
	\item \emph{negative\begin{equation}
	A \vect{x} =  \lambda \vect{x}.
\end{equation}
 definite} if for all non-zero $\vect{x} \in \R^m$, $q(\vect{x}) > 0$;
	\item \emph{indefinite} if $q(\vect{x})$ takes on both positive and negative values.  
\end{enumerate}

\begin{example}
Let $q(\vect{h}) = -h_1^2 + 2h_2^2 - h_3^2$ such that the matrix associated with~$q$ is
\[
	\begin{pmatrix}
	-1 & 0 & 0 \\
	0  & 2 & 0 \\
    0  & 0 & -1 
	\end{pmatrix}.
\]
The $q(0, 1, 0) = 2$ and $q(1, 0, 1) = -2$. Thus the quardatic form $q$ is indefinite.
\end{example}    

\begin{example}
The quadratic form $q(h_1, h_2, h_3) = h_1^2 + h_2^2 + 4h_3^2 + 2h_1h_2 - 4h_2h_3 - 4h_1 h_3$
is positive definite, since we may re-write $q(h_1, h_2, h_3)$ as $(h_1 + h_2 - 2h_3)^2$
which is strictly positive for all $(h_1, h_2, h_3) \in \R^3 \setminus \{\vect{0}\}$.
\end{example}
To characterize the definiteness of quadratic forms, we make use of the following 
properties of real symmetric matrices.
\begin{lemma}
The eigenvectors of a real symmetric matrix~$A$ are real.
\end{lemma}
\begin{proof}
Suppose that $A$ is an $m \times m$ matrix and suppose that some eigenvalue~$\lambda$ 
is complex. Let $\vect{x}$ be a eigenvector corresponding to it, which can be a complex 
vector. Then 
\begin{equation}\label{pf:real_eigen1}
	A \vect{x} =  \lambda \vect{x}.
\end{equation}
Taking complex conjugates on each side, we get:
\begin{equation}\label{pf:real_eigen2}
	A \conj{\vect{x}} =  \conj{\lambda} \conj{\vect{x}}.
\end{equation}
Pre-multiply~(\ref{pf:real_eigen1})  by $\conj{\vect{x}}$ 
and~(\ref{pf:real_eigen2}) by $\trans{\vect{x}}$ 
to obtain:
\begin{eqnarray}
	\trans{\conj{\vect{x}}} A \vect{x} =  \trans{\conj{\vect{x}}} \lambda \vect{x} \label{pf:real_eigen3}\\
	\trans{\vect{x} }A \conj{\vect{x}} =  \trans{\vect{x}} \conj{\lambda} \conj{\vect{x}} \label{pf:real_eigen4}.
\end{eqnarray}
Take the transpose of equation~(\ref{pf:real_eigen4}), we obtain: 
$\trans{\conj{\vect{x}}} A \vect{x} =  \trans{\conj{\vect{x}}} \conj{\lambda} \vect{x}$, 
where we made use of the fact that $A$ is symmetric. Now subtracting this equation from~(\ref{pf:real_eigen3}),
we obtain: 
\[
\trans{\vect{x}} (\lambda - \conj{\lambda}) \conj{\vect{x}} = 0.
\]
Since $\trans{\vect{x}} \conj{\vect{x}}$ is the sum of products of complex conjugates, 
it is not zero unless each component of $\vect{x}$ is zero. Since this is not the case 
($\vect{x}$ is an eigenvector), we must have $\lambda = \conj{\lambda}$ and hence $\lambda$
is real. 
\end{proof} 

