\chapter{Using Neural Networks to Recognize Handwritten Digits}

\begin{exercise} Consider a network of perceptrons. Suppose that
we multiply all weights and biases by a positive constant $c > 0$. Show 
that the behaviour of the network does not change.
\end{exercise}
\begin{solution}
First consider a single perceptron. Assume that weights and bias are $w_1, \ldots, w_n$ and $b$, respectively. Then $\sum_i w_i \cdot x_i + b$ and 
$c \cdot (\sum_i w_i \cdot x_i + b)$ and exactly the same sign and hence multiplying the weights and the bias by $c$ will not change the behaviour of this single perceptron. Now if all perceptrons in a network have their weights 
and biases multiplied by $c > 0$, then each individual perceptron behaves as before and hence the network behaves as before.
\end{solution}
