\chapter{Using Neural Networks to Recognize Handwritten Digits}

\begin{exercise} Consider a network of perceptrons. Suppose that
we multiply all weights and biases by a positive constant $c > 0$. Show 
that the behaviour of the network does not change.
\end{exercise}
\begin{solution}
First consider a single perceptron. Assume that weights and bias are $w_1,
\ldots, w_n$ and $b$, respectively. Then $\sum_i w_i \cdot x_i + b$ and $c
\cdot (\sum_i w_i \cdot x_i + b)$ have exactly the same sign and hence
multiplying the weights and the bias by $c$ will not change the behaviour of
this single perceptron. Now if all perceptrons in a network have their weights
and biases multiplied by $c > 0$, then each individual perceptron behaves as
before and hence the network behaves as before.  
\end{solution}

\begin{exercise}
Suppose that we have network of perceptrons with a chosen input value
$\vect{x}$. We wonâ€™t need the actual input value, we just need the input to
have been fixed. Suppose the weights and biases are such that all $\vect{w}
\cdot \vect{x} + b \neq 0$ for the input $\vect{x}$ to any particular
perceptron in the network. Now replace all the perceptrons in the network by
sigmoid neurons, and multiply the weights and biases of the network by a
positive constant $c > 0$. Show that in the limit as $c \to \infty$, the 
behaviour of this network of sigmoid neurons is exactly the same as the network
of perceptrons. How can this fail when $\vect{w} \cdot \vect{x} + b = 0$ for
one of the perceptrons?
\end{exercise}
\begin{solution}
As in the previous exercise, first consider a single perceptron in the network. 
When this is replaced by a sigmoid neuron, and we let $c \to \infty$, 
$c \cdot (\vect{w} \cdot \vect{x} + b)$ tends to either $+ \infty$ or $- \infty$
depending on whether $\vect{w} \cdot \vect{x} + b$ is positive or negative. The 
upshot is that the output of the sigmoid neuron matches that of the perceptron 
it replaced. Thus when every sigmoid neuron behaves as the perceptron it replaced, 
the network as a whole behaves similarly. 

This works as long as $\vect{w} \cdot \vect{x} + b \neq 0$. If this is zero, the 
output of the sigmoid neuron is ``stuck'' at $1/2$ irrespective of the value of $c$, 
while the perceptron outputs a $0$. The outputs do not match and the behaviour 
of the sigmoid network may be different. 
\end{solution}
