\chapter{Using Neural Networks to Recognize Handwritten Digits}

\begin{exercise} Consider a network of perceptrons. Suppose that
we multiply all weights and biases by a positive constant $c > 0$. Show 
that the behaviour of the network does not change.
\end{exercise}
\begin{solution}
First consider a single perceptron. Assume that weights and bias are $w_1,
\ldots, w_n$ and $b$, respectively. Then $\sum_i w_i \cdot x_i + b$ and $c
\cdot (\sum_i w_i \cdot x_i + b)$ have exactly the same sign and hence
multiplying the weights and the bias by $c$ will not change the behaviour of
this single perceptron. Now if all perceptrons in a network have their weights
and biases multiplied by $c > 0$, then each individual perceptron behaves as
before and hence the network behaves as before.  
\end{solution}

\begin{exercise}
Suppose that we have network of perceptrons with a chosen input value
$\vect{x}$. We wonâ€™t need the actual input value, we just need the input to
have been fixed. Suppose the weights and biases are such that all $\vect{w}
\cdot \vect{x} + b \neq 0$ for the input $\vect{x}$ to any particular
perceptron in the network. Now replace all the perceptrons in the network by
sigmoid neurons, and multiply the weights and biases of the network by a
positive constant $c > 0$. Show that in the limit as $c \to \infty$, the 
behaviour of this network of sigmoid neurons is exactly the same as the network
of perceptrons. How can this fail when $\vect{w} \cdot \vect{x} + b = 0$ for
one of the perceptrons?
\end{exercise}
\begin{solution}
As in the previous exercise, first consider a single perceptron in the network. 
When this is replaced by a sigmoid neuron, and we let $c \to \infty$, 
$c \cdot (\vect{w} \cdot \vect{x} + b)$ tends to either $+ \infty$ or $- \infty$
depending on whether $\vect{w} \cdot \vect{x} + b$ is positive or negative. The 
upshot is that the output of the sigmoid neuron matches that of the perceptron 
it replaced. Thus when every sigmoid neuron behaves as the perceptron it replaced, 
the network as a whole behaves similarly. 

This works as long as $\vect{w} \cdot \vect{x} + b \neq 0$. If this is zero, the 
output of the sigmoid neuron is ``stuck'' at $1/2$ irrespective of the value of $c$, 
while the perceptron outputs a $0$. The outputs do not match and the behaviour 
of the sigmoid network may be different. 
\end{solution}

\begin{exercise}
There is a way of determining the bitwise representation of a digit by adding
an extra layer to the three-layer network given in the book.  The extra layer
converts the output of the previous layer in binary representation. Find a set
of weights and biases for the new output layer.  Assume that the first three
layers of neurons are such that the correct output in the third layer (i.e.,
the old output layer) has activation at least $0.99$, and incorrect outputs
have activation less than $0.01$.
\end{exercise}
\begin{solution}
Label the neurons of the third layer (the old output layer) as $0, 1, \ldots, 9$
and the neurons from the new output layer as $0', 1', 2', 3'$ with the interpretation 
that neuron $0'$ is the least significant bit and $3'$ is the most significant 
bit of the number represented by the output layer. The weight of the 
connection between the $i$th neuron from the third layer and the $j$th neuron 
of the output layer is $w_{i j}$, where $i \in \{0, \ldots, 9\}$ and 
$j \in \{0', 1', 2', 3'\}$. The bias of the $j$th output neuron is $b_j$. Denote 
the output of the $i$th neuron from the third layer as $x_i$. Then the input to the
final layer may be represented as:
\[
    \begin{pmatrix}
        z_0 \\
        z_1 \\
        z_2 \\
        z_3 
    \end{pmatrix} = 
    \begin{pmatrix}
        w_{0 0} & w_{1 0} & \ldots & w_{9 0} & b_0 \\
        w_{0 1} & w_{1 1} & \ldots & w_{9 1} & b_1 \\
        w_{0 2} & w_{1 2} & \ldots & w_{9 2} & b_2 \\
        w_{0 3} & w_{1 3} & \ldots & w_{9 3} & b_3 
    \end{pmatrix} 
    \begin{pmatrix}
        x_0 \\
        \vdots \\
        x_9 \\
        1
    \end{pmatrix}
\]
Now we would like $z_0$ to be $1$ when the number is $1, 3, 5, 7, 9$ 
and $0$ otherwise. To be able to do this, first set 
\[ 
w_{1 0} = w_{3 0} = w_{5 0} = w_{7 0} = w_{9 0} = +1 
\]
and the remaining weights of the inputs to $0'$ to $-1$. Set $b_0 = 0$.
Now if the third layer represents $k \in \{1, 3, 5, 7, 9\}$, we would have 
$w_{k 0} > 0.99$ and $w_{j 0} < 0.01$ for all $j \neq k$. With these 
weights, we would have $z_0 > 0.99 - 9 \times 0.01 = 0.90$. If the third layer 
represents a number $k \notin \{1, 3, 5, 7, 9\}$, then 
$z_0 < -0.99 + 9 \times 0.01 = -0.90$. We can amplify this phenomenon by
multiplying all these weights by a large positive constant. This would lead
the sigmoid neuron $0'$ to output a $1$ for the digits $1, 3, 5, 7, 9$ and 
a $0$ for the remaining digits. 

We can use a similar strategy for the remaining neurons of the fourth layer. For 
example, the second most significant bit $1'$ must be a $1$ for the digits 
$2, 3, 6, 7, 9$ and a $0$ for the remaining digits. We would then set 
\[ 
w_{2 1} = w_{3 1} = w_{6 1} = w_{7 1} = w_{9 1} = +1 
\]
and the remaining weights to $-1$. The bias $b_1$ is set to $0$. 
\end{solution}

\begin{exercise}
Let $C(v_1, \ldots, v_m) \colon \R^m \rightarrow \R$ be a differentiable 
function. Then $\Delta C \approx \nabla C \cdot \Delta \vect{v}$. Constrain
$\norm{\Delta \vect{v}} = \epsilon$, where $\epsilon > 0$ is a small fixed 
real. Show that the choice of $\Delta \vect{v}$ that minimizes 
$\nabla C \cdot \Delta \vect{v}$ is $\Delta \vect{v} = - \eta \nabla C$, 
where $\eta = \epsilon / \norm{\nabla C}$.
\end{exercise}
\begin{solution}

\end{solution}
