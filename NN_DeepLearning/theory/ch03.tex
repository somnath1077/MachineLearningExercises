\chapter{Improving the Way Neural Networks Learn}

\section{The Cross-Entropy Cost Function}

\begin{exercise}
Show that the cross-entropy function is minimized when $\sigma(z) = y$ for all inputs.
\end{exercise}
\begin{solution}
The cross-entropy function is defined as:
\[
    C = - \frac{1}{m} \sum_{i = 1}^m [y_i \ln (a_i) + (1 - y_i) \ln (1 - a_i)],
\]
where the $y_i$s are fixed and the $a_i$s are the ``variables.'' Now $\partial C / \partial a_i$
is given by:
\[
    \frac{\partial C}{\partial a_i} = \frac{1}{m} \frac{a_i - y_i}{a_i (1 - a_i)}. 
\]
At an extremum point of $C$, each component of the gradient $\nabla_a C$ will be zero. This 
happens when $a_i = y_i$ for all~$1 \leq i \leq m$. 

As a side note, the function $H(y) = - [y \ln(y) + (1 - y) \ln (1 - y)]$ for $y \in (0, 1)$ 
is called the binary entropy function and behaves as shown in Figure~\ref{fig:binary_entropy}. 
\end{solution}

\begin{figure}[ht]
\begin{center}
\includegraphics[scale=0.60]{entropy.png}
\end{center}
\caption{The Binary Entropy Function}
\label{fig:binary_entropy}
\end{figure}

\begin{exercise}
Partial derivatives of the cross-entropy cost function in multi-layer
networks.
\end{exercise}
\begin{solution}
The cross-entropy function for a single training example~$x$ for the last
layer~$L$ of the network is defined as:
\[
    C(x) = - \sum_j \left [ y_j \ln (a_j^L) + (1 - y_j) \ln(1 - a_j^L) \right ],
\]
where the sum is over all neurons~$j$ in layer~$L$. To recap notation,  
\[
a_j^L = \sigma(z_j^L) = \sum_k w_{j k}^L a_k^{L - 1} + b_j^L.
\]
For this training example~$x$,
\begin{align*}
    \frac{\partial C(x)}{\partial z_j^L} 
        & = - \frac{y_j}{a_j^L} \cdot \sigma' (z_j^L) + \frac{1 - y_j}{1 - a_j^L} \cdot \sigma' (z_j^L) \\
        & = \frac{-y_j + y_j a_j^L + a_j^L - y_j a_j^L}{a_j^L (1 - a_j^L)} \cdot \sigma' (z_j^L) \\
        & = a_j^L - y_j.
\end{align*}
The last equality follows since $\sigma' (z_j^L) = a_j^L (1 - a_j^L)$.

Again, for this single training example~$x$, $\partial C (x) / \partial w_{j k}^L$ 
is given by:
\begin{align*}
    \frac{\partial C (x)}{\partial w_{j k}^L} 
        & = \frac{\partial C}{\partial z_j^L} \cdot \frac{\partial z_j^L}{\partial w_{j k}^L} \\
        & = (a_j^L - y_j) \cdot a_k^{L - 1}. 
\end{align*}
For $n$ training examples, the cost function is defined as $\frac{1}{n} \sum_{x} C(x)$ and 
this derivative is: 
\[
    \frac{\partial C}{\partial w_{j k}^L} = \frac{1}{n} \sum_x a_k^{L - 1} (a_j^L - y_j).
\]
If we were to replace $C(x)$ by the usual quadratic cost $\frac{1}{2}(y_j - a_j^L)^2$, then 
the same derivative would have been:
\[
    \frac{1}{n} \sum_x a_k^{L - 1} (a_j^L - y_j) \cdot \sigma' (z_j^L).
\]
\end{solution}

\subsection{Deriving the Cross-Entropy Function}

Given a single neuron with $r$ input weights $w_1, \ldots, w_r$ and bias~$b$,
and a single input $\vect{x} = \trans{(x_1, \ldots, x_r)}$, we would like the
cost function $C$ to depend on the weights and the bias as follows:
\begin{align}
    \frac{\partial C}{\partial w_j} & = x_j (a - y) \\
    \frac{\partial C}{\partial b}   & = a - y, \label{eqn:cost_bias}
\end{align}
where $a = \sigma(\sum_{j} w_j x_j + b)$ and $y$ is the desired output 
corresponding to $\vect{x}$. 

Using the chain rule, we obtain:
\begin{align}
    \frac{\partial C}{\partial b} & = 
        \frac{\partial C}{\partial a} \cdot 
        \frac{\partial a}{\partial z} \cdot 
        \frac{\partial z}{\partial b} \nonumber \\
        & = \frac{\partial C}{\partial a} \cdot \sigma'(z) \nonumber \\ 
        & = \frac{\partial C}{\partial a} \cdot a (1 - a) \label{eqn:cross_entropy} 
\end{align}
From equations~(\ref{eqn:cost_bias} and~\ref{eqn:cross_entropy}), we obtain:
\[
    \frac{\partial C}{\partial a} = \frac{a - y}{a (1 - a)} = 
        \frac{1}{1 - a} - y \left ( \frac{1}{1 - a} + \frac{1}{a}\right ).
\]
Integrating both sides wrt~$a$, we obtain:
\[
    C = - [(1 - y) \ln (1 - a) + y \ln (a)] + \text{ a constant}.
\]

\section{Softmax}

Consider a classification problem, where labelled examples take the form 
$(\vect{x}, \vect{y})$, where $\vect{x} \in \R^m$ and $\vect{y} \in \{0, 1\}^J$ 
denotes to which of the $J$ classes $\vect{x}$ belongs to. 
In such cases, it makes sense to have the last 
layer of the neural network to have $J$ neurons with the softmax activation:
\begin{align*}
    z_j^L & = \sum_k w_{j k}^L a_{k}^{L - 1} + b_j \\
    a_j^L & = \frac{e^{z_j^L}}{ \sum_{i = 1}^J e^{z_i^L}}
\end{align*}

The cost associated with the input $(\vect{x}, \vect{y})$ where $y_r = 1$ 
is defined as the negative log-likelihood of the activation $a_{r}^L$:
\[
    C(\vect{x}, \vect{y}) = - \ln a_{r}^L.
\]

The partial derivatives $\partial C / \partial b_j^L$ and   $\partial C / \partial w_{j k}^L$
can be computed easily as follows. Depending on whether the index $j$ and the class index~$r$
are the same or not, we have two cases for each partial derivative.
\begin{align}
    \frac{\partial C}{\partial b_{r}^L} 
        & =   - \frac{1}{a_{r}^L} \cdot 
            \left [ \frac{e^{z_{r}^L}}{ \sum_{i = 1}^J e^{z_i^L}} - 
                    \left ( \frac{e^{z_{r}^L}}{ \sum_{i = 1}^J e^{z_i^L}} \right )^2 
            \right ] = - 1 + a_{r}^L \\
\frac{\partial C}{\partial b_j^L} 
      & = 
    - \frac{1}{a_{r}^L} \cdot \left [ 0 - \frac{e^{z_{r}^L} e^{z_j^L}}{ \left ( \sum_{i = 1}^J e^{z_i^L} \right )^2} 
                            \right ] = a_j^L. 
\end{align}
The first equation is when the index $r = j$ and the second when $r \neq j$. 
These two expressions can be summarized into one: 
\begin{equation}
\frac{\partial C}{\partial b_j^L} = a_{j}^L - y_j.
\end{equation}

Similarly, the partial derivative expression for  $\partial C / \partial w_{j k}^L$
can be written as:
\begin{equation}
\frac{\partial C}{\partial w_{j k}^L} = a_{k}^{L - 1} \cdot (a_{j}^L - y_j). 
\end{equation}

\begin{exercise}
Where does the ``softmax'' name come from? Consider the following variant 
of the softmax function:
\[
    a_j^L =  \frac{e^{c z_j^L}}{ \sum_{i = 1}^J e^{c z_i^L}},
\]
where $c$ is a positive constant. What is the limit of $a_j^L$ 
as $c \to \infty$?
\end{exercise}
\begin{solution}
Let $z_r^L = \max_i \{z_i^L\}$. We could then write the modified softmax
function as:
\[
    a_j^L =  \frac{e^{c (z_j^L - z_r^L)}}{ 1 + \sum_{i \neq r} e^{c (z_i^L - z_r^L)}}. 
\]
If $j = r$, then the numerator is $1$ and the denominator approaches $1$ as $c \to \infty$
and $a_j^L \to 1$. On the other hand, if $j \neq r$, the numerator $\to 0$ as $c \to \infty$;
the denominator in any case approaches~$1$ and hence~$a_j^L \to 0$. The point here is 
that $a_j^L = 1$ if $z_j^L$ is the maximum and $a_j^L = 0$ otherwise. 
\end{solution}

\section{Regularization}

Regularization is a set of methods used to prevent overfitting and improve 
generalization performance. The typical way to do this is to tack on a 
``regularization term'' in the loss function. The most commonly used of these 
are the $L_1$-regularization and the 
$L_2$-regularization. These are defined in terms of the $L_1$ and $L_2$-norms 
respectively. For a real $p \geq 1$, the $L_p$-norm of a vector $w \in \R^d$ is defined as  
\begin{equation}
    \norm{w} = \left ( |w_1|^p + \cdots + |w_d|^p \right )^{1/p}.
\end{equation}
Let us assume that $\mathscr{L}(w)$ is the original loss function. Then the 
$L_2$-regularized loss functions looks like this:
\[
    \mathscr{L}(w) + \frac{\alpha}{2} \cdot \norm{w}_2^2,
\]
where $\alpha > 0$ is real number that specifies the ``strength'' of the 
regularization. 

\subsection{$L_2$-regularization Analysis}
Let $w^{\star}$ be an optimal solution to $\mathscr{L}(w)$, the unregularized 
loss function. Given a vector $w \in \R^{d}$, write $w$ as $w^{\star} + \epsilon$, 
where $\epsilon$ is viewed as the ``error.'' Use a second-order Taylor series  
expansion to write down the loss~$\mathscr{L}$ at the point $w^{\star} + \epsilon$ 
as:
\begin{equation}
\label{eqn:loss_taylor_series}
\mathscr{L}(w^{\star} + \epsilon) = 
    \mathscr{L}(w^{\star}) + 
    \epsilon^{T} \cdot \nabla \mathscr{L}(w^{\star}) +
    \frac{1}{2} \epsilon^{T} \cdot H_{\mathscr{L}} \cdot \epsilon,
\end{equation}
where $H_{\mathscr{L}}$ represents the Hessian of $\mathscr{L}$ evaluated at 
$w^{\star}$. Recall that the Hessian, in this case, is a $d \times d$ matrix 
whose $(i, j)$th term is given by:
\[
    \left . \frac{\partial^2 \mathscr{L}(w)}{\partial w_i \partial w_j} \right |_{w^{\star}}.
\]
We assume that the loss function is continuous at $w^{\star}$ so that the order 
of partial derivatives does not matter. In this case, the Hessian is a symmetric 
$d \times d$ matrix. In particular, note that this implies that all eigenvalues 
are real. 

Since $\nabla \mathscr{L}(w^{\star}) = 0$, we may rewrite 
Equation~\ref{eqn:loss_taylor_series} as:
\[
    \mathscr{L}(w) = \mathscr{L}(w^{\star}) + 
    \frac{1}{2} (w - w^{\star})^{T} \cdot H_{\mathscr{L}} \cdot (w - w^{\star}).
\]
Taking gradients wrt~$w$, we obtain that:
\begin{equation}
\label{eqn:loss_func_grad}
    \nabla \mathscr{L}(w) = 0 + H_{\mathscr{L}} \cdot (w - w^{\star}).
\end{equation}

Consider the $L_2$-regularized loss function 
$\tilde{\mathscr{L}}(w) = \mathscr{L}(w) + \frac{\alpha}{2} \cdot \norm{w}^2$ .
Let $\tilde{w}$ be an optimum solution to this regularized loss function. Then 
\begin{equation}
\label{eqn:reg_loss_func_grad}
    0 = \nabla \tilde{\mathscr{L}}(\tilde{w}) = 
        \nabla \mathscr{L}(\tilde{w}) 
        + \alpha \tilde{w}.
\end{equation}
Using the expression from Equation~\ref{eqn:loss_func_grad} in the above equation, 
we get:
\begin{gather}
H_{\mathscr{L}} \cdot (\tilde{w} - w^{\star}) + \alpha \tilde{w} = 0 \notag \\
\begin{split}
(H_{\mathscr{L}} + \alpha \cdot I) \tilde{w} & = H_{\mathscr{L}} \cdot w^{\star} \\
\therefore \tilde{w} & = (H_{\mathscr{L}} + \alpha \cdot I)^{-1} \cdot H_{\mathscr{L}} \cdot w^{\star}.
\end{split}
\end{gather}
Here wer are assuming that the symmetric matrix $H_{\mathscr{L}} + \alpha \cdot I$
is invertible. In fact, we are going make a stronger assumption about $H_{\mathscr{L}}$
that it is positive definite. Recall that this means that the eigenvalues of 
$H_{\mathscr{L}}$ are all strictly positive. 
