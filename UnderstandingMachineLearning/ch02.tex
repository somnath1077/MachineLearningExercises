\chapter{Finite Hypothesis Classes}

\section{Setting}

Consider a classification problem in which the learning algorithm receives as input
a sequence of training examples $S = \{(x_1, y_1), \ldots, (x_m, y_m)\}$,
where $x_i \in \dom$ and $y_i \in \range = \{0, 1\}$. The sequence of training
examples is drawn iid from some unknown distribution $\dist$ and labeled by some
target function $f \colon \dom \rightarrow \range$.

Given a hypothesis $h \colon \dom \rightarrow \range$, the \emph{true error}
or the \emph{generalization error} $\generror{h}$ of the hypothesis is defined to be:
\begin{align*}
    \generror{h} & = \Prtwo{x \sim \mathcal{D}}{h(x) \neq f(x)} \\
                 & = 1 \cdot \Prtwo{x \sim \mathcal{D}}{h(x) \neq f(x)} +
                     0 \cdot \Prtwo{x \sim \mathcal{D}}{h(x) = f(x)} \\
                 & = \Exptwo{\mathcal{D}}{\lvert h - f \rvert}.
\end{align*}
The generalization error is the expected number of points in the domain at which
the hypothesis~$h$ differs from the true labeling function~$f$, the expectation being
calculated wrt the distribution~$\dist$. The learning
algorithm does not directly know the true error. What it can calculate
is the \emph{training error} $L_{S}(h)$ which is defined as:
\[
    L_S (h) = \frac{1}{m} \sum_{i = 1}^{m} 1_{h(x_i) \neq y_i},
\]
where $1_{h(x_i) \neq y_i} =  1$ if $h(x_i) \neq y_i$ and $0$ otherwise. Note that
the training error is the expected number of points~$x$ at which $h(x)$ differs from
the true label~$y$ wrt the uniform distribution.

The empirical risk minimization (ERM) paradigm is a learning paradigm where the learner,
when given a training sample $S$, comes up with a hypothesis $h_S$ that minimizes
the training error on $S$. That is, $h_S = \argmin_{h} L_S(h)$. We may
constrain the ERM algorithm to a specific class of hypotheses~$\hypclass$. In this
case, the ERM algorithm is forced to output an element $h_S \in \hypclass$ where
$h_S = \argmin_{h \in \hypclass} L_S(h)$.


\section{Finite Hypothesis Classes}

We assume that we have a finite hypothesis class $\hypclass$. For a training
sample $S$ labeled by some function $f: \dom \rightarrow \range$, let $h_S$
be the hypothesis output by $\erm{\hypclass}$ when applied to $S$. In this chapter,
we assume that there exists a hypothesis $h^{\star}$ such that
$\generror{h^{\star}} = 0$. In particular, this means that for any hypothesis $h_S$
output by the $\erm{\hypclass}$, we have $L_S(h_S) = 0$ (since
$\min_{h \in \hypclass} L_S(h) = 0$).

We wish to upper bound the generalization error $\generror{h_S}$ of the hypothesis
output by $\erm{\hypclass}$ on the sample $S$. Note that the hypothesis~$h_S$ depends
on the sample $S$ and the only way we can connect the sample $S$ to the distribution
$\dist$ is by making an assumption on how it was generated. The standard assumption
is that $S$ is generated i.i.d according to the distribution $\dist$. Thus $h_S$ is
a random variable and is potentially different for different training samples.
Therefore when we talk about bounding the generalization error of the output
hypothesis, we must talk about the fraction of samples for which this is
possible.

Thus bounding the generalization error of the output hypothesis $h$ involves
the specification of two parameters:
a \emph{confidence parameter} $\delta \in (0, 1)$ that measures for what fraction
of training samples $\erm{\hypclass}$ outputs a hypothesis $h$ that does \emph{not}
generalize well; and an \emph{accuracy parameter}~$\epsilon \in (0, 1)$ that
specifies how badly off the generalization accuracy is. In other ``words,'' we
want to bound the probability that $\generror{h_S} > \epsilon$, since this represents
the case where the hypothesis~$h_S$ does not generalize well. The probability is
calculated w.r.t samples that are chosen i.i.d from the distribution~$\dist$.
That is, we are looking at the following condition:
\begin{equation}
    \Prtwo{S \sim \dist^m}{\generror{h_S} > \epsilon} < \delta.
\end{equation}

To help analyze this probability, define $\hypclass_{B}$, the set of \emph{bad hypotheses},
to be the subset of hypotheses of $\hypclass$ that have a generalization error
exceeding $\epsilon$.
\[
    \hypclass_{B} = \{h \in \hypclass \colon \generror{h} > \epsilon\}.
\]
Also define the set $M$ of \emph{misleading samples} to be:
\[
    M = \{S \colon \exists h \in \hypclass_{B} \text{ s.t. } L_S(h) = 0\}.
\]
That is, a sample is misleading if there exists a bad hypothesis that appears to be
good on it.

Since we assume realizability, any hypothesis~$h_S$ picked up by $\erm{\hypclass}$
in response to a training sample~$S$ has zero training error. Now if
$\generror{h} > \epsilon$, then we must have $h_S \in \hypclass_{B}$. Moreover, since
$L_S(h_S) = 0$, it must be that $S \in M$. This shows that
$\{S \colon \generror{h_S} > \epsilon\} \subseteq M$ and that:
\begin{align}
    \label{ineq:main_prob_ineq}
    \Prtwo{S \sim \dist^m}{\generror{h_S} > \epsilon}
    & \leq \Prtwo{S \sim \dist^m}{S \in M} \nonumber \\
    & = \Prtwo{S \sim \dist^m}{\exists h \in \hypclass_{B} \colon L_S(h) = 0} \nonumber \\
    & \leq \sum_{h \in \hypclass_{B}} \Prtwo{S \sim \dist^m}{L_S(h) = 0},
\end{align}
where the last inequality follows from the union bound.

Fix a hypothesis $h \in \hypclass_{B}$. Then
\[
    \Prtwo{S \sim \dist^m}{L_S(h) = 0} =
    \prod_{i = 1}^{m} \Prtwo{x_i \sim \dist}{h(x_i) = f(x_i)}.
\]
Since $\generror{h} > \epsilon$, we have that:
\[
    \Prtwo{S \sim \dist^m}{L_S(h) = 0} = (1 - \epsilon)^m \leq e^{-\epsilon m }.
\]
From~(\ref{ineq:main_prob_ineq}), we get that:
\begin{equation}
    \Prtwo{S \sim \dist^m}{\generror{h_S} > \epsilon}
        \leq \lvert \hypclass_{B} \rvert \cdot e^{-\epsilon m }
        \leq \lvert \hypclass \rvert \cdot e^{-\epsilon m }.
\end{equation}
Let us require that $\lvert \hypclass \rvert \cdot e^{-\epsilon m } < \delta$.
This implies that as long as
$m > (1/\epsilon) \cdot \log (\lvert \hypclass \rvert / \delta)$, we have that
$\Prtwo{S \sim \dist^m}{\generror{h} > \epsilon} < \delta$. In words, the hypothesis
$h_S$ output by $\erm{\hypclass}$ is such that its generalization error on at
least $1 - \delta$ fraction of the samples is at most $\epsilon$.


